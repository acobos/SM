<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>2 The logistic regression model (LRM) | Statistical Modeling for Clinical Researchers</title>
  <meta name="description" content="2 The logistic regression model (LRM) | Statistical Modeling for Clinical Researchers" />
  <meta name="generator" content="bookdown 0.29 and GitBook 2.6.7" />

  <meta property="og:title" content="2 The logistic regression model (LRM) | Statistical Modeling for Clinical Researchers" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="2 The logistic regression model (LRM) | Statistical Modeling for Clinical Researchers" />
  
  
  

<meta name="author" content="Albert Cobos" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="the-general-linear-model-glm.html"/>

<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
<script src="libs/htmlwidgets-1.6.1/htmlwidgets.js"></script>
<script src="libs/rglWebGL-binding-0.110.2/rglWebGL.js"></script>
<link href="libs/rglwidgetClass-0.110.2/rgl.css" rel="stylesheet" />
<script src="libs/rglwidgetClass-0.110.2/rglClass.src.js"></script>
<script src="libs/rglwidgetClass-0.110.2/utils.src.js"></script>
<script src="libs/rglwidgetClass-0.110.2/buffer.src.js"></script>
<script src="libs/rglwidgetClass-0.110.2/subscenes.src.js"></script>
<script src="libs/rglwidgetClass-0.110.2/shaders.src.js"></script>
<script src="libs/rglwidgetClass-0.110.2/textures.src.js"></script>
<script src="libs/rglwidgetClass-0.110.2/projection.src.js"></script>
<script src="libs/rglwidgetClass-0.110.2/mouse.src.js"></script>
<script src="libs/rglwidgetClass-0.110.2/init.src.js"></script>
<script src="libs/rglwidgetClass-0.110.2/pieces.src.js"></script>
<script src="libs/rglwidgetClass-0.110.2/draw.src.js"></script>
<script src="libs/rglwidgetClass-0.110.2/controls.src.js"></script>
<script src="libs/rglwidgetClass-0.110.2/selection.src.js"></script>
<script src="libs/rglwidgetClass-0.110.2/rglTimer.src.js"></script>
<script src="libs/rglwidgetClass-0.110.2/pretty.src.js"></script>
<script src="libs/rglwidgetClass-0.110.2/axes.src.js"></script>
<script src="libs/rglwidgetClass-0.110.2/animation.src.js"></script>
<!--html_preserve--><script type = "text/plain" id = "rgl-vertex-shader">
#line 2 1
// File 1 is the vertex shader
#ifdef GL_ES
#ifdef GL_FRAGMENT_PRECISION_HIGH
precision highp float;
#else
precision mediump float;
#endif
#endif

attribute vec3 aPos;
attribute vec4 aCol;
uniform mat4 mvMatrix;
uniform mat4 prMatrix;
varying vec4 vCol;
varying vec4 vPosition;

#ifdef NEEDS_VNORMAL
attribute vec3 aNorm;
uniform mat4 normMatrix;
varying vec4 vNormal;
#endif

#if defined(HAS_TEXTURE) || defined (IS_TEXT)
attribute vec2 aTexcoord;
varying vec2 vTexcoord;
#endif

#ifdef FIXED_SIZE
uniform vec3 textScale;
#endif

#ifdef FIXED_QUADS
attribute vec3 aOfs;
#endif

#ifdef IS_TWOSIDED
#ifdef HAS_NORMALS
varying float normz;
uniform mat4 invPrMatrix;
#else
attribute vec3 aPos1;
attribute vec3 aPos2;
varying float normz;
#endif
#endif // IS_TWOSIDED

#ifdef FAT_LINES
attribute vec3 aNext;
attribute vec2 aPoint;
varying vec2 vPoint;
varying float vLength;
uniform float uAspect;
uniform float uLwd;
#endif


void main(void) {
  
#ifndef IS_BRUSH
#if defined(NCLIPPLANES) || !defined(FIXED_QUADS) || defined(HAS_FOG)
  vPosition = mvMatrix * vec4(aPos, 1.);
#endif
  
#ifndef FIXED_QUADS
  gl_Position = prMatrix * vPosition;
#endif
#endif // !IS_BRUSH
  
#ifdef IS_POINTS
  gl_PointSize = POINTSIZE;
#endif
  
  vCol = aCol;
  
#ifdef NEEDS_VNORMAL
  vNormal = normMatrix * vec4(-aNorm, dot(aNorm, aPos));
#endif
  
#ifdef IS_TWOSIDED
#ifdef HAS_NORMALS
  /* normz should be calculated *after* projection */
  normz = (invPrMatrix*vNormal).z;
#else
  vec4 pos1 = prMatrix*(mvMatrix*vec4(aPos1, 1.));
  pos1 = pos1/pos1.w - gl_Position/gl_Position.w;
  vec4 pos2 = prMatrix*(mvMatrix*vec4(aPos2, 1.));
  pos2 = pos2/pos2.w - gl_Position/gl_Position.w;
  normz = pos1.x*pos2.y - pos1.y*pos2.x;
#endif
#endif // IS_TWOSIDED
  
#ifdef NEEDS_VNORMAL
  vNormal = vec4(normalize(vNormal.xyz/vNormal.w), 1);
#endif
  
#if defined(HAS_TEXTURE) || defined(IS_TEXT)
  vTexcoord = aTexcoord;
#endif
  
#if defined(FIXED_SIZE) && !defined(ROTATING)
  vec4 pos = prMatrix * mvMatrix * vec4(aPos, 1.);
  pos = pos/pos.w;
  gl_Position = pos + vec4(aOfs*textScale, 0.);
#endif
  
#if defined(IS_SPRITES) && !defined(FIXED_SIZE)
  vec4 pos = mvMatrix * vec4(aPos, 1.);
  pos = pos/pos.w + vec4(aOfs,  0.);
  gl_Position = prMatrix*pos;
#endif
  
#ifdef FAT_LINES
  /* This code was inspired by Matt Deslauriers' code in 
   https://mattdesl.svbtle.com/drawing-lines-is-hard */
  vec2 aspectVec = vec2(uAspect, 1.0);
  mat4 projViewModel = prMatrix * mvMatrix;
  vec4 currentProjected = projViewModel * vec4(aPos, 1.0);
  currentProjected = currentProjected/currentProjected.w;
  vec4 nextProjected = projViewModel * vec4(aNext, 1.0);
  vec2 currentScreen = currentProjected.xy * aspectVec;
  vec2 nextScreen = (nextProjected.xy / nextProjected.w) * aspectVec;
  float len = uLwd;
  vec2 dir = vec2(1.0, 0.0);
  vPoint = aPoint;
  vLength = length(nextScreen - currentScreen)/2.0;
  vLength = vLength/(vLength + len);
  if (vLength > 0.0) {
    dir = normalize(nextScreen - currentScreen);
  }
  vec2 normal = vec2(-dir.y, dir.x);
  dir.x /= uAspect;
  normal.x /= uAspect;
  vec4 offset = vec4(len*(normal*aPoint.x*aPoint.y - dir), 0.0, 0.0);
  gl_Position = currentProjected + offset;
#endif
  
#ifdef IS_BRUSH
  gl_Position = vec4(aPos, 1.);
#endif
}
</script>
<script type = "text/plain" id = "rgl-fragment-shader">
#line 2 2
// File 2 is the fragment shader
#ifdef GL_ES
#ifdef GL_FRAGMENT_PRECISION_HIGH
precision highp float;
#else
precision mediump float;
#endif
#endif
varying vec4 vCol; // carries alpha
varying vec4 vPosition;
#if defined(HAS_TEXTURE) || defined (IS_TEXT)
varying vec2 vTexcoord;
uniform sampler2D uSampler;
#endif

#ifdef HAS_FOG
uniform int uFogMode;
uniform vec3 uFogColor;
uniform vec4 uFogParms;
#endif

#if defined(IS_LIT) && !defined(FIXED_QUADS)
varying vec4 vNormal;
#endif

#if NCLIPPLANES > 0
uniform vec4 vClipplane[NCLIPPLANES];
#endif

#if NLIGHTS > 0
uniform mat4 mvMatrix;
#endif

#ifdef IS_LIT
uniform vec3 emission;
uniform float shininess;
#if NLIGHTS > 0
uniform vec3 ambient[NLIGHTS];
uniform vec3 specular[NLIGHTS]; // light*material
uniform vec3 diffuse[NLIGHTS];
uniform vec3 lightDir[NLIGHTS];
uniform bool viewpoint[NLIGHTS];
uniform bool finite[NLIGHTS];
#endif
#endif // IS_LIT

#ifdef IS_TWOSIDED
uniform bool front;
varying float normz;
#endif

#ifdef FAT_LINES
varying vec2 vPoint;
varying float vLength;
#endif

void main(void) {
  vec4 fragColor;
#ifdef FAT_LINES
  vec2 point = vPoint;
  bool neg = point.y < 0.0;
  point.y = neg ? (point.y + vLength)/(1.0 - vLength) :
                 -(point.y - vLength)/(1.0 - vLength);
#if defined(IS_TRANSPARENT) && defined(IS_LINESTRIP)
  if (neg && length(point) <= 1.0) discard;
#endif
  point.y = min(point.y, 0.0);
  if (length(point) > 1.0) discard;
#endif // FAT_LINES
  
#ifdef ROUND_POINTS
  vec2 coord = gl_PointCoord - vec2(0.5);
  if (length(coord) > 0.5) discard;
#endif
  
#if NCLIPPLANES > 0
  for (int i = 0; i < NCLIPPLANES; i++)
    if (dot(vPosition, vClipplane[i]) < 0.0) discard;
#endif
    
#ifdef FIXED_QUADS
    vec3 n = vec3(0., 0., 1.);
#elif defined(IS_LIT)
    vec3 n = normalize(vNormal.xyz);
#endif
    
#ifdef IS_TWOSIDED
    if ((normz <= 0.) != front) discard;
#endif
    
#ifdef IS_LIT
    vec3 eye = normalize(-vPosition.xyz/vPosition.w);
    vec3 lightdir;
    vec4 colDiff;
    vec3 halfVec;
    vec4 lighteffect = vec4(emission, 0.);
    vec3 col;
    float nDotL;
#ifdef FIXED_QUADS
    n = -faceforward(n, n, eye);
#endif
    
#if NLIGHTS > 0
    for (int i=0;i<NLIGHTS;i++) {
      colDiff = vec4(vCol.rgb * diffuse[i], vCol.a);
      lightdir = lightDir[i];
      if (!viewpoint[i])
        lightdir = (mvMatrix * vec4(lightdir, 1.)).xyz;
      if (!finite[i]) {
        halfVec = normalize(lightdir + eye);
      } else {
        lightdir = normalize(lightdir - vPosition.xyz/vPosition.w);
        halfVec = normalize(lightdir + eye);
      }
      col = ambient[i];
      nDotL = dot(n, lightdir);
      col = col + max(nDotL, 0.) * colDiff.rgb;
      col = col + pow(max(dot(halfVec, n), 0.), shininess) * specular[i];
      lighteffect = lighteffect + vec4(col, colDiff.a);
    }
#endif
    
#else // not IS_LIT
    vec4 colDiff = vCol;
    vec4 lighteffect = colDiff;
#endif
    
#ifdef IS_TEXT
    vec4 textureColor = lighteffect*texture2D(uSampler, vTexcoord);
#endif
    
#ifdef HAS_TEXTURE
#ifdef TEXTURE_rgb
    vec4 textureColor = lighteffect*vec4(texture2D(uSampler, vTexcoord).rgb, 1.);
#endif
    
#ifdef TEXTURE_rgba
    vec4 textureColor = lighteffect*texture2D(uSampler, vTexcoord);
#endif
    
#ifdef TEXTURE_alpha
    vec4 textureColor = texture2D(uSampler, vTexcoord);
    float luminance = dot(vec3(1.,1.,1.), textureColor.rgb)/3.;
    textureColor =  vec4(lighteffect.rgb, lighteffect.a*luminance);
#endif
    
#ifdef TEXTURE_luminance
    vec4 textureColor = vec4(lighteffect.rgb*dot(texture2D(uSampler, vTexcoord).rgb, vec3(1.,1.,1.))/3., lighteffect.a);
#endif
    
#ifdef TEXTURE_luminance_alpha
    vec4 textureColor = texture2D(uSampler, vTexcoord);
    float luminance = dot(vec3(1.,1.,1.),textureColor.rgb)/3.;
    textureColor = vec4(lighteffect.rgb*luminance, lighteffect.a*textureColor.a);
#endif
    
    fragColor = textureColor;

#elif defined(IS_TEXT)
    if (textureColor.a < 0.1)
      discard;
    else
      fragColor = textureColor;
#else
    fragColor = lighteffect;
#endif // HAS_TEXTURE
    
#ifdef HAS_FOG
    // uFogParms elements: x = near, y = far, z = fogscale, w = (1-sin(FOV/2))/(1+sin(FOV/2))
    // In Exp and Exp2: use density = density/far
    // fogF will be the proportion of fog
    // Initialize it to the linear value
    float fogF;
    if (uFogMode > 0) {
      fogF = (uFogParms.y - vPosition.z/vPosition.w)/(uFogParms.y - uFogParms.x);
      if (uFogMode > 1)
        fogF = mix(uFogParms.w, 1.0, fogF);
      fogF = fogF*uFogParms.z;
      if (uFogMode == 2)
        fogF = 1.0 - exp(-fogF);
      // Docs are wrong: use (density*c)^2, not density*c^2
      // https://gitlab.freedesktop.org/mesa/mesa/-/blob/master/src/mesa/swrast/s_fog.c#L58
      else if (uFogMode == 3)
        fogF = 1.0 - exp(-fogF*fogF);
      fogF = clamp(fogF, 0.0, 1.0);
      gl_FragColor = vec4(mix(fragColor.rgb, uFogColor, fogF), fragColor.a);
    } else gl_FragColor = fragColor;
#else
    gl_FragColor = fragColor;
#endif // HAS_FOG
    
}
</script><!--/html_preserve-->
<script src="libs/CanvasMatrix4-0.110.2/CanvasMatrix.src.js"></script>
<script src="libs/font-awesome-5.13.0/js/script.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="1" data-path="the-general-linear-model-glm.html"><a href="the-general-linear-model-glm.html"><i class="fa fa-check"></i><b>1</b> The general linear model (GLM)</a>
<ul>
<li class="chapter" data-level="1.1" data-path="the-general-linear-model-glm.html"><a href="the-general-linear-model-glm.html#data"><i class="fa fa-check"></i><b>1.1</b> Data</a></li>
<li class="chapter" data-level="1.2" data-path="the-general-linear-model-glm.html"><a href="the-general-linear-model-glm.html#fitting-a-glm"><i class="fa fa-check"></i><b>1.2</b> Fitting a GLM</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="the-general-linear-model-glm.html"><a href="the-general-linear-model-glm.html#predicted-values"><i class="fa fa-check"></i><b>1.2.1</b> Predicted values</a></li>
<li class="chapter" data-level="1.2.2" data-path="the-general-linear-model-glm.html"><a href="the-general-linear-model-glm.html#residuals"><i class="fa fa-check"></i><b>1.2.2</b> Residuals</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="the-general-linear-model-glm.html"><a href="the-general-linear-model-glm.html#inference"><i class="fa fa-check"></i><b>1.3</b> Inference on the model coefficients</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="the-general-linear-model-glm.html"><a href="the-general-linear-model-glm.html#CIC"><i class="fa fa-check"></i><b>1.3.1</b> Confidence intervals (CI) for the model coefficients</a></li>
<li class="chapter" data-level="1.3.2" data-path="the-general-linear-model-glm.html"><a href="the-general-linear-model-glm.html#CIP"><i class="fa fa-check"></i><b>1.3.2</b> Confidence intervals (CI) for the model predictions</a></li>
<li class="chapter" data-level="1.3.3" data-path="the-general-linear-model-glm.html"><a href="the-general-linear-model-glm.html#tests-on-the-model-coefficients"><i class="fa fa-check"></i><b>1.3.3</b> Tests on the model coefficients</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="the-general-linear-model-glm.html"><a href="the-general-linear-model-glm.html#goodness-of-fit"><i class="fa fa-check"></i><b>1.4</b> Goodness-of-fit</a></li>
<li class="chapter" data-level="1.5" data-path="the-general-linear-model-glm.html"><a href="the-general-linear-model-glm.html#assessing-assumptions"><i class="fa fa-check"></i><b>1.5</b> Assessing assumptions</a>
<ul>
<li class="chapter" data-level="1.5.1" data-path="the-general-linear-model-glm.html"><a href="the-general-linear-model-glm.html#fixing-non-linearity-polynomials"><i class="fa fa-check"></i><b>1.5.1</b> Fixing non-linearity: polynomials</a></li>
<li class="chapter" data-level="1.5.2" data-path="the-general-linear-model-glm.html"><a href="the-general-linear-model-glm.html#fixing-heteroscedasticity"><i class="fa fa-check"></i><b>1.5.2</b> Fixing heteroscedasticity</a></li>
<li class="chapter" data-level="1.5.3" data-path="the-general-linear-model-glm.html"><a href="the-general-linear-model-glm.html#influence-measures"><i class="fa fa-check"></i><b>1.5.3</b> Influence measures</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="the-general-linear-model-glm.html"><a href="the-general-linear-model-glm.html#probability-intervals-or-prediction-intervals"><i class="fa fa-check"></i><b>1.6</b> Probability intervals (or prediction intervals)</a></li>
<li class="chapter" data-level="1.7" data-path="the-general-linear-model-glm.html"><a href="the-general-linear-model-glm.html#models-with-two-continuous-predictors"><i class="fa fa-check"></i><b>1.7</b> Models with two continuous predictors</a></li>
<li class="chapter" data-level="1.8" data-path="the-general-linear-model-glm.html"><a href="the-general-linear-model-glm.html#comparing-models"><i class="fa fa-check"></i><b>1.8</b> Comparing models</a></li>
<li class="chapter" data-level="1.9" data-path="the-general-linear-model-glm.html"><a href="the-general-linear-model-glm.html#confounding"><i class="fa fa-check"></i><b>1.9</b> Confounding</a></li>
<li class="chapter" data-level="1.10" data-path="the-general-linear-model-glm.html"><a href="the-general-linear-model-glm.html#interaction-effect-modification"><i class="fa fa-check"></i><b>1.10</b> Interaction (effect modification)</a></li>
<li class="chapter" data-level="" data-path="the-general-linear-model-glm.html"><a href="the-general-linear-model-glm.html#resources"><i class="fa fa-check"></i>Resources</a></li>
<li class="chapter" data-level="" data-path="the-general-linear-model-glm.html"><a href="the-general-linear-model-glm.html#exercises"><i class="fa fa-check"></i>Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="the-logistic-regression-model-lrm.html"><a href="the-logistic-regression-model-lrm.html"><i class="fa fa-check"></i><b>2</b> The logistic regression model (LRM)</a>
<ul>
<li class="chapter" data-level="2.1" data-path="the-logistic-regression-model-lrm.html"><a href="the-logistic-regression-model-lrm.html#logit-transformation"><i class="fa fa-check"></i><b>2.1</b> Logit transformation</a></li>
<li class="chapter" data-level="2.2" data-path="the-logistic-regression-model-lrm.html"><a href="the-logistic-regression-model-lrm.html#fitting-a-lrm"><i class="fa fa-check"></i><b>2.2</b> Fitting a LRM</a></li>
<li class="chapter" data-level="2.3" data-path="the-logistic-regression-model-lrm.html"><a href="the-logistic-regression-model-lrm.html#interpretation-of-model-coefficients"><i class="fa fa-check"></i><b>2.3</b> Interpretation of model coefficients</a></li>
<li class="chapter" data-level="2.4" data-path="the-logistic-regression-model-lrm.html"><a href="the-logistic-regression-model-lrm.html#inference-on-the-model-coefficients"><i class="fa fa-check"></i><b>2.4</b> Inference on the model coefficients</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="the-logistic-regression-model-lrm.html"><a href="the-logistic-regression-model-lrm.html#confidence-intervals-ci-for-the-model-coefficients"><i class="fa fa-check"></i><b>2.4.1</b> Confidence intervals (CI) for the model coefficients</a></li>
<li class="chapter" data-level="2.4.2" data-path="the-logistic-regression-model-lrm.html"><a href="the-logistic-regression-model-lrm.html#tests-on-the-model-coefficients-1"><i class="fa fa-check"></i><b>2.4.2</b> Tests on the model coefficients</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="the-logistic-regression-model-lrm.html"><a href="the-logistic-regression-model-lrm.html#assessment-of-fit"><i class="fa fa-check"></i><b>2.5</b> Assessment of fit</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="the-logistic-regression-model-lrm.html"><a href="the-logistic-regression-model-lrm.html#deviance-likelihood-ratio-test-and-aic"><i class="fa fa-check"></i><b>2.5.1</b> Deviance, likelihood ratio test and AIC</a></li>
<li class="chapter" data-level="2.5.2" data-path="the-logistic-regression-model-lrm.html"><a href="the-logistic-regression-model-lrm.html#hosmer-lemeshow-test"><i class="fa fa-check"></i><b>2.5.2</b> Hosmer-Lemeshow test</a></li>
<li class="chapter" data-level="2.5.3" data-path="the-logistic-regression-model-lrm.html"><a href="the-logistic-regression-model-lrm.html#other-measures-of-fit"><i class="fa fa-check"></i><b>2.5.3</b> Other measures of fit</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="the-logistic-regression-model-lrm.html"><a href="the-logistic-regression-model-lrm.html#model-predictions-and-classification"><i class="fa fa-check"></i><b>2.6</b> Model predictions and classification</a>
<ul>
<li class="chapter" data-level="2.6.1" data-path="the-logistic-regression-model-lrm.html"><a href="the-logistic-regression-model-lrm.html#predictions-and-confusion-matrix"><i class="fa fa-check"></i><b>2.6.1</b> Predictions and confusion matrix</a></li>
<li class="chapter" data-level="2.6.2" data-path="the-logistic-regression-model-lrm.html"><a href="the-logistic-regression-model-lrm.html#roc-curves-and-auc"><i class="fa fa-check"></i><b>2.6.2</b> ROC curves and AUC</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="the-logistic-regression-model-lrm.html"><a href="the-logistic-regression-model-lrm.html#comparing-models-1"><i class="fa fa-check"></i><b>2.7</b> Comparing models</a></li>
<li class="chapter" data-level="2.8" data-path="the-logistic-regression-model-lrm.html"><a href="the-logistic-regression-model-lrm.html#confounding-1"><i class="fa fa-check"></i><b>2.8</b> Confounding</a></li>
<li class="chapter" data-level="2.9" data-path="the-logistic-regression-model-lrm.html"><a href="the-logistic-regression-model-lrm.html#interaction-effect-modification-1"><i class="fa fa-check"></i><b>2.9</b> Interaction (effect modification)</a></li>
<li class="chapter" data-level="2.10" data-path="the-logistic-regression-model-lrm.html"><a href="the-logistic-regression-model-lrm.html#model-diagnostics"><i class="fa fa-check"></i><b>2.10</b> Model diagnostics</a></li>
<li class="chapter" data-level="" data-path="the-logistic-regression-model-lrm.html"><a href="the-logistic-regression-model-lrm.html#resources-1"><i class="fa fa-check"></i>Resources</a></li>
<li class="chapter" data-level="" data-path="the-logistic-regression-model-lrm.html"><a href="the-logistic-regression-model-lrm.html#exercises-1"><i class="fa fa-check"></i>Exercises</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistical Modeling for Clinical Researchers</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="the-logistic-regression-model-lrm" class="section level1 hasAnchor" number="2">
<h1><span class="header-section-number">2</span> The logistic regression model (LRM)<a href="the-logistic-regression-model-lrm.html#the-logistic-regression-model-lrm" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>The <code>burn1000</code> dataset in the <code>aplore3</code> package has data on 1000 cases of burns, including the hospital discharge status (Dead or Alive), demographic variables, the total burn surface area (as percentage of body surface area), and weather or not the burn involved inhalation injury or flames (see <code>?burn1000</code>). After loading the required packages, we rename the outcome variable, save the result as <code>d</code>, and get a summary of selected variables:</p>
<div class="sourceCode" id="cb82"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb82-1"><a href="the-logistic-regression-model-lrm.html#cb82-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb82-2"><a href="the-logistic-regression-model-lrm.html#cb82-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(aplore3)</span>
<span id="cb82-3"><a href="the-logistic-regression-model-lrm.html#cb82-3" aria-hidden="true" tabindex="-1"></a>d <span class="ot">&lt;-</span> burn1000 <span class="sc">%&gt;%</span> </span>
<span id="cb82-4"><a href="the-logistic-regression-model-lrm.html#cb82-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rename</span>(<span class="at">outcome =</span> death) </span>
<span id="cb82-5"><a href="the-logistic-regression-model-lrm.html#cb82-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-6"><a href="the-logistic-regression-model-lrm.html#cb82-6" aria-hidden="true" tabindex="-1"></a><span class="fu">select</span>(d, <span class="sc">-</span>id, <span class="sc">-</span>facility, <span class="sc">-</span> race) <span class="sc">%&gt;%</span> <span class="fu">summary</span>()</span></code></pre></div>
<pre><code>  outcome         age           gender         tbsa       inh_inj   flame    
 Alive:850   Min.   : 0.10   Female:295   Min.   : 0.10   No :878   No :471  
 Dead :150   1st Qu.:10.85   Male  :705   1st Qu.: 2.50   Yes:122   Yes:529  
             Median :31.95                Median : 6.00                      
             Mean   :33.29                Mean   :13.54                      
             3rd Qu.:51.23                3rd Qu.:16.00                      
             Max.   :89.70                Max.   :98.00                      </code></pre>
<p>The previous output shows that most patients survived, age ranged from less than one to almost 90 years and males predominate; relatively few burns involved inhalation injury but almost half of them involve flame; and the total burn surface area varied widely, ranging from less than 1% to 98%.</p>
<p>Suppose we want to predict the outcome “Dead” from the total burn surface area. We could do it by coding the outcome as an indicator (0 for “Alive”, 1 for “Death”) and fitting a straight line (GLM), as done in figure <a href="the-logistic-regression-model-lrm.html#fig:2-glm-lrm">2.1</a> (left). The outcome can take values 0 or 1 only, but we could interpret the predictions of the linear model for any given <code>tbsa</code> as outcome probabilities. However, for extreme values of <code>tbsa</code> like 1% or 90%, the linear model predicts a value out of the allowed range for a probability: less than 0 for values of <code>tbsa</code> close to 0, and more than 1 for values of <code>tbsa</code> close to 1. Clearly, the linear model in not appropriate.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:2-glm-lrm"></span>
<img src="_main_files/figure-html/2-glm-lrm-1.png" alt="Linear and logistic functions to model the probability of death from the total body surface area" width="1056" />
<p class="caption">
Figure 2.1: Linear and logistic functions to model the probability of death from the total body surface area
</p>
</div>
<p><br />
</p>
<p>To circumvent the problem of getting predicted probabilities out of the allowed 0 to 1 range, we should use an asymptotic function that never crosses the 0 and 1 boundaries for a probability. One such function is the <em>logistic function</em> shown in figure <a href="the-logistic-regression-model-lrm.html#fig:2-glm-lrm">2.1</a> (right). This function is:</p>
<p><span class="math display">\[\pi = \frac{e^{\beta_0 + \beta_1 X}}{1 \enspace + \enspace e^{\beta_0 + \beta_1 X}}\]</span></p>
<p>where <span class="math inline">\(\pi\)</span> denotes the probability of observing the event of interest. Note that the event of interest is <em>one of the levels of a binary variable</em>, (such as <code>outcome = "Death"</code>). Then, a more explicit notation for this probability would be <span class="math inline">\(\pi(Death)\)</span>, but for the sake of simplicity and the clarity of formulas, we use simply <span class="math inline">\(\pi\)</span>.</p>
<p>The logistic function can be used as well with more than one explanatory, variable. A logistic function with <span class="math inline">\(p&gt;1\)</span> explanatory variables is:</p>
<p><span class="math display" id="eq:2-logistic">\[\begin{equation}
\pi = \frac{e^{\beta_0 + \beta_1 X + \beta_2 X + ... + \beta_p}}{1 \enspace + \enspace e^{\beta_0 + \beta_1 X + \beta_2 X + ... + \beta_p}}
\tag{2.1}
\end{equation}\]</span></p>
<div id="logit-transformation" class="section level2 hasAnchor" number="2.1">
<h2><span class="header-section-number">2.1</span> Logit transformation<a href="the-logistic-regression-model-lrm.html#logit-transformation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The logistic function in <a href="the-logistic-regression-model-lrm.html#eq:2-logistic">(2.1)</a> is a quite complex function of the explanatory variables <span class="math inline">\(X_1, X_2,..., X_p\)</span>. Fortunately, it can become much simpler in terms of a transformation of <span class="math inline">\(\pi\)</span> called <em>logit</em> or <em>log odds</em>, which is the natural logarithm of the odds of death. It can be proved that equation <a href="the-logistic-regression-model-lrm.html#eq:2-logistic">(2.1)</a> is equivalent to:</p>
<p><span class="math display" id="eq:2-logit">\[\begin{equation}
ln(\frac{\pi}{1-\pi}) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + ... \beta_p X_p
\tag{2.2}
\end {equation}\]</span></p>
<p>The left hand side of equation <a href="the-logistic-regression-model-lrm.html#eq:2-logit">(2.2)</a> is called the <em>logit</em> of <span class="math inline">\(\pi\)</span>, or the <em>log-odds</em> of <span class="math inline">\(\pi\)</span>, it is linearly related to the explanatory variables, and that is why the right hand side of the equation is sometimes called the <em>linear predictor</em>.</p>
<p>The LRM assumes that the relation between a probability <span class="math inline">\(\pi\)</span> and a linear predictor of explanatory variables <span class="math inline">\(X\)</span> is described by the logistic function of equation <a href="the-logistic-regression-model-lrm.html#eq:2-logistic">(2.1)</a>; or equivalently, it assumes that the logit of this probability is described by the linear function of one or more explanatory variables of equation <a href="the-logistic-regression-model-lrm.html#eq:2-logit">(2.2)</a>. Explanatory variables can be either quantitative or categorical, but categorical variables with <span class="math inline">\(k\)</span> levels are entered as <span class="math inline">\(k-1\)</span> binary indicators, just as in GLMs.</p>
<p>The LRM also assumes that the binary outcome has a <a href="https://en.wikipedia.org/wiki/Bernoulli_distribution">Bernouilli</a> distribution of parameter <span class="math inline">\(\pi\)</span>, and this implies that in any set of <span class="math inline">\(n\)</span> observations having the same values of the explanatory variables, the number of cases with the event of interest follow a <a href="https://en.wikipedia.org/wiki/Binomial_distribution">binomial distribution</a> of parameters <span class="math inline">\(\pi\)</span> and <span class="math inline">\(n\)</span>.</p>
<p>The coefficients of a LRM are estimated by a method called <em>maximum likelihood</em> (ML). This method looks for the values of the coefficients that would make the probability of the observed data as high as possible. The estimates obtained with this method are called ML estimates. ML works well in large samples, but not so well in small samples. In the case of LRMs, a rule of thumb to determine if a sample is large enough to get reliable ML estimates is to verify that we have 10 to 20 events and non-events per model coefficient. In the case of the burns dataset, we have 150 events (deaths) and 850 non-events (alive). This means that we could reliably fit models with 150/20 = 7 coefficients at most, to be on the safe side.</p>
</div>
<div id="fitting-a-lrm" class="section level2 hasAnchor" number="2.2">
<h2><span class="header-section-number">2.2</span> Fitting a LRM<a href="the-logistic-regression-model-lrm.html#fitting-a-lrm" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Suppose we want to fit a LRM to express the probability of death as a function of <code>tbsa</code> and <code>flame</code>. This can be done with function <code>glm()</code>, as done below. Two things should be noted:</p>
<ul>
<li><p>Because we want to model the probability of “Dead”, and this is <em>a level</em> of the <code>outcome</code> variable, the left hand side of the formula passed as first argument to <code>glm()</code> is <code>outcome == "Dead"</code><a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a></p></li>
<li><p>The second argument <code>family = binomial</code> is needed if we want to fit a LRM. Function <code>glm()</code> owes its name to the fact that it can be used to fit <em>any</em> type of <em>gerneralized</em> linear model, and LRM is just <em>one</em> of these types. The <code>family</code> argument indicates the <code>glm()</code> function <em>which</em> type of generalized lineal model we want to fit.</p></li>
</ul>
<p>The remaining arguments to this function are similar as those for the <code>lm()</code> functions we used in the previous chapter to fit GLMs.</p>
<p>After fitting the model, we can print the estimated coefficients with <code>coef()</code>:</p>
<div class="sourceCode" id="cb84"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb84-1"><a href="the-logistic-regression-model-lrm.html#cb84-1" aria-hidden="true" tabindex="-1"></a>m_tf <span class="ot">&lt;-</span> <span class="fu">glm</span>(outcome <span class="sc">==</span> <span class="st">&quot;Dead&quot;</span> <span class="sc">~</span> tbsa <span class="sc">+</span> flame, <span class="at">family =</span> binomial, <span class="at">data =</span> d)</span>
<span id="cb84-2"><a href="the-logistic-regression-model-lrm.html#cb84-2" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(m_tf) </span></code></pre></div>
<pre><code>(Intercept)        tbsa    flameYes 
-4.10581359  0.07811869  1.26715789 </code></pre>
<p>What the output shows is the estimates of the coefficients in the linear predictor. Thus, the estimated model is (rounding the coefficients to the fourth decimal):</p>
<p><span class="math inline">\(ln(\frac{\pi}{1-\pi})\)</span> <code>= -3.3451 + 0.0854 tbsa</code></p>
</div>
<div id="interpretation-of-model-coefficients" class="section level2 hasAnchor" number="2.3">
<h2><span class="header-section-number">2.3</span> Interpretation of model coefficients<a href="the-logistic-regression-model-lrm.html#interpretation-of-model-coefficients" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The linear predictor of a LRM allows to compute the log odds of <span class="math inline">\(\pi\)</span>, according to equation <a href="the-logistic-regression-model-lrm.html#eq:2-logit">(2.2)</a>. If all <span class="math inline">\(X\)</span> variables were equal to 0 in this equation, the resulting log odds would be <span class="math inline">\(\beta_0\)</span>. Consequently, the intercept <span class="math inline">\(\beta_0\)</span> can be interpreted as the value of the log odds when all <span class="math inline">\(X\)</span> variables 0. This will be of no particular interest if the value 0 is impossible for some <span class="math inline">\(X\)</span> variable(s), which is the case in our example: <code>tbsa</code> = 0 cannot happen, since it would imply no burn at all. Therefore the intercept in this model has no useful interpretation.</p>
<p>The remaining coefficients of equation <a href="the-logistic-regression-model-lrm.html#eq:2-logit">(2.2)</a> (<span class="math inline">\(\beta_1, \beta_2, ..., \beta_p\)</span>) quantify the <em>change</em> that would result in the log-odds for a unit increment in the corresponding <span class="math inline">\(X\)</span> variable, <em>while keeping constant all other</em> <span class="math inline">\(X\)</span> <em>variables</em>. To se this, let’s compare the log odds that will result when <span class="math inline">\(X_1\)</span> takes the value <span class="math inline">\(k\)</span>, and when it takes the value <span class="math inline">\(k+1\)</span>:</p>
<ul>
<li><p><span class="math inline">\(X_1 = k: \qquad \qquad ln \left( \frac{\pi|X_1 = k}{1-\pi|X_1 = k} \right) = \beta_0 + \beta_1 k + \beta_2 X_2 + ... + \beta_p X_p\)</span></p></li>
<li><p><span class="math inline">\(X_1 = k+1: \qquad ln \left( \frac{\pi|X_1 = k+1}{1-\pi|X_1 = k+1} \right) = \beta_0 + \beta_1 (k+1) + \beta_2 X_2 + ... + \beta_p X_p\)</span></p></li>
</ul>
<p><span class="math inline">\(\qquad \qquad \qquad \qquad \qquad \qquad \qquad \quad = \beta_0 + \beta_1 k + \beta_1 + \beta_2 X_2 + ... + \beta_p X_p\)</span>,</p>
<p>where <span class="math inline">\(\pi|X_1 = k\)</span> is the probability of the outcome, <em>given</em> <span class="math inline">\(X_1 = k\)</span>.</p>
<p>Comparing these two results, we see they only differ by <span class="math inline">\(\beta_1\)</span>. Therefore <span class="math inline">\(\beta_1\)</span> can be written as the difference between the two log odds, and because a difference of logarithms is the logarithm of a ratio, we get:</p>
<p><span class="math display">\[\beta1 \quad = \quad ln \left( \frac{\pi|X_1 = k+1}{1-\pi|X_1 = k+1} \right) - ln \left( \frac{\pi|X_1 = k}{1-\pi|X_1 = k} \right) \quad = \quad ln \left( \frac{\frac{\pi|X_1 = k+1}{1-\pi|X_1 = k+1}}{\frac{\pi|X_1 = k}{1-\pi|X_1 = k}} \right)\]</span></p>
<p>Note that the last term of the previous equation is the logarithm of an odds ratio (OR) that compares the odds of the outcome event if <span class="math inline">\(X_1 = k+1\)</span> (top) vs <span class="math inline">\(X_1 = k\)</span> (bottom). So, this is the logarithm of the OR corresponding to a unit increase in variable <span class="math inline">\(X_1\)</span>. Therefore, the exponential of <span class="math inline">\(\beta_1\)</span> is the OR corresponding to such an increment in <span class="math inline">\(X_1\)</span>:</p>
<p><span class="math display" id="eq:2-exp-coef">\[\begin{equation}
e^{\beta1} \quad = \quad \frac{\frac{\pi|X_1 = k+1}{1-\pi|X_1 = k+1}}{\frac{\pi|X_1 = k}{1-\pi|X_1 = k}}
\tag{2.3}
\end{equation}\]</span></p>
<p>Here we exponentiate the coefficients of model <code>m_tf</code> fitted in the previous section, using the <code>exp()</code> function:</p>
<div class="sourceCode" id="cb86"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb86-1"><a href="the-logistic-regression-model-lrm.html#cb86-1" aria-hidden="true" tabindex="-1"></a><span class="fu">exp</span>(<span class="fu">coef</span>(m_tf))</span></code></pre></div>
<pre><code>(Intercept)        tbsa    flameYes 
 0.01647661  1.08125099  3.55074659 </code></pre>
<p>The exponential of the intercept is the log odds when <code>tbsa</code> is zero, but as commented above this has no particular interest (since <code>tbsa</code> &gt; 0, by definition).</p>
<p>The exponential of the coefficient for <code>tbsa</code> is the OR corresponding a unit increase in <code>tbsa</code>. This implies that the odds of death increases by a factor of 1.0813 (rounded to 4 decimals) for each 1% increment in <code>tbsa</code>. If we want to compute the OR for an increment other than 1, we just need to raise the OR we got to the desired number of units. For instance, the OR corresponding to a 10% increment in <code>tbsa</code> is:</p>
<div class="sourceCode" id="cb88"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb88-1"><a href="the-logistic-regression-model-lrm.html#cb88-1" aria-hidden="true" tabindex="-1"></a><span class="fu">exp</span>(<span class="fu">coef</span>(m_tf))[<span class="dv">2</span>]<span class="sc">^</span><span class="dv">10</span>       </span></code></pre></div>
<pre><code>    tbsa 
2.184063 </code></pre>
<p>Rounding this result to the second decimal, we may say that the odds of death increases by a factor of 2.18 for each 10% increment in <code>tbsa</code>.</p>
<p>Similarly, the OR corresponding to a 20% increment in <code>tbsa</code> is:</p>
<div class="sourceCode" id="cb90"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb90-1"><a href="the-logistic-regression-model-lrm.html#cb90-1" aria-hidden="true" tabindex="-1"></a><span class="fu">exp</span>(<span class="fu">coef</span>(m_tf))[<span class="dv">2</span>]<span class="sc">^</span><span class="dv">20</span>        </span></code></pre></div>
<pre><code>    tbsa 
4.770132 </code></pre>
<p>which implies that the odds of death increases by a factor of 4.77 for each 20% increment in <code>tbsa</code>.</p>
</div>
<div id="inference-on-the-model-coefficients" class="section level2 hasAnchor" number="2.4">
<h2><span class="header-section-number">2.4</span> Inference on the model coefficients<a href="the-logistic-regression-model-lrm.html#inference-on-the-model-coefficients" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The equation we obtained in section <a href="the-logistic-regression-model-lrm.html#fitting-a-lrm">2.2</a> was fitted from sample data and is therefore an estimate of the population equation (i.e., the equations we would get if we could fit the model to the whole population, not just to a finite sample). We can compute 95% CIs for the model coefficients to see what are likely values of these coefficients in the population; we can also produce Wald tests to assess the null hypothesis that a specific coefficient is zero in the population. In both cases, the functions we need to use are the same as those we used for a GLM.</p>
<div id="confidence-intervals-ci-for-the-model-coefficients" class="section level3 hasAnchor" number="2.4.1">
<h3><span class="header-section-number">2.4.1</span> Confidence intervals (CI) for the model coefficients<a href="the-logistic-regression-model-lrm.html#confidence-intervals-ci-for-the-model-coefficients" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Confidence intervals for the coefficients of a LRM can be obtained with function <code>confint()</code>:</p>
<div class="sourceCode" id="cb92"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb92-1"><a href="the-logistic-regression-model-lrm.html#cb92-1" aria-hidden="true" tabindex="-1"></a><span class="fu">confint</span>(m_tf)</span></code></pre></div>
<pre><code>                  2.5 %     97.5 %
(Intercept) -4.69616358 -3.5907614
tbsa         0.06518979  0.0923746
flameYes     0.71873320  1.8604831</code></pre>
<p>The output shows the limits of the 95% CI for all model parameters. For instance, the 95% CI for the effect of <code>tbsa</code> is 0.0652 to 0.0924 (rounded to four decimals). However, this is a 95% CI for the effect of <code>tbsa</code> on the logit scale (equation <a href="the-logistic-regression-model-lrm.html#eq:2-logit">(2.2)</a>). To obtain a 95% CI for the OR of <code>tbsa</code> we need to exponentiate this result, just as we did in section <a href="the-logistic-regression-model-lrm.html#interpretation-of-model-coefficients">2.3</a> to get the point estimate of the OR:</p>
<div class="sourceCode" id="cb94"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb94-1"><a href="the-logistic-regression-model-lrm.html#cb94-1" aria-hidden="true" tabindex="-1"></a><span class="fu">exp</span>(<span class="fu">confint</span>(m_tf))</span></code></pre></div>
<pre><code>                  2.5 %     97.5 %
(Intercept) 0.009130237 0.02757732
tbsa        1.067361577 1.09677560
flameYes    2.051832300 6.42684078</code></pre>
<p>From the output we can say that the 95% CI for the OR of <code>tbsa</code> ranges from 1.07 to 1.1 (rounded to the second decimal).</p>
<p>You may find practical the following code to merge the point estimates and their correponding CI’s for the exponentiated model coefficients. We first use <code>rbind()</code> to merge point estimates and CI’s, and then round to the desired number of decimals:</p>
<div class="sourceCode" id="cb96"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb96-1"><a href="the-logistic-regression-model-lrm.html#cb96-1" aria-hidden="true" tabindex="-1"></a>res <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="at">Estimate =</span> <span class="fu">exp</span>(<span class="fu">coef</span>(m_tf)), <span class="fu">exp</span>(<span class="fu">confint</span>(m_tf))) <span class="sc">%&gt;%</span> <span class="fu">round</span>(<span class="dv">2</span>)</span>
<span id="cb96-2"><a href="the-logistic-regression-model-lrm.html#cb96-2" aria-hidden="true" tabindex="-1"></a>res</span></code></pre></div>
<pre><code>            Estimate 2.5 % 97.5 %
(Intercept)     0.02  0.01   0.03
tbsa            1.08  1.07   1.10
flameYes        3.55  2.05   6.43</code></pre>
<p>If we want to compute the OR for a different increment in <code>tbsa</code>, say 10 years, then we need to raise the results to the power of 10. Here we do it by subsetting the second row of <code>res</code>, then raise it to 10, and finally round to two decimals:</p>
<div class="sourceCode" id="cb98"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb98-1"><a href="the-logistic-regression-model-lrm.html#cb98-1" aria-hidden="true" tabindex="-1"></a>res[<span class="dv">2</span>,]<span class="sc">^</span><span class="dv">10</span>  <span class="sc">%&gt;%</span> <span class="fu">round</span>(<span class="dv">2</span>)       </span></code></pre></div>
<pre><code>Estimate    2.5 %   97.5 % 
    2.16     1.97     2.59 </code></pre>
<p>Thus, the OR <em>for a 10 year increase</em> in <code>tbsa</code> is 2.16 [95% CI: 1.97, 2.59].</p>
</div>
<div id="tests-on-the-model-coefficients-1" class="section level3 hasAnchor" number="2.4.2">
<h3><span class="header-section-number">2.4.2</span> Tests on the model coefficients<a href="the-logistic-regression-model-lrm.html#tests-on-the-model-coefficients-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Wald test on the model coefficients are produced with function <code>summary()</code>:</p>
<div class="sourceCode" id="cb100"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb100-1"><a href="the-logistic-regression-model-lrm.html#cb100-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(m_tf)</span></code></pre></div>
<pre><code>
Call:
glm(formula = outcome == &quot;Dead&quot; ~ tbsa + flame, family = binomial, 
    data = d)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.6751  -0.4115  -0.2559  -0.1916   2.8444  

Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -4.105814   0.280726 -14.626  &lt; 2e-16 ***
tbsa         0.078119   0.006928  11.276  &lt; 2e-16 ***
flameYes     1.267158   0.289756   4.373 1.22e-05 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 845.42  on 999  degrees of freedom
Residual deviance: 516.68  on 997  degrees of freedom
AIC: 522.68

Number of Fisher Scoring iterations: 6</code></pre>
<p>The <code>Coefficients</code> part of the output shows the estimates of the model coefficients and the result of the Wald test for each coefficient. These asses the null hypothesis that the corresponding coefficient in the population is equal to zero (and therefore the OR = 1, since <span class="math inline">\(e \ ^ 0 = 1\)</span>). Consequently, low <em>p</em> values allow to reject the null hypothesis and conclude the corresponding coefficient in the population <em>is not</em> equal zero (and therefore the OR <em>is not</em> equal to 1). In this case, the Wald tests for the coefficients of <code>tbsa</code> and <code>flameYes</code> give a very low p-value, providing evidence that they are both related to the outcome (since the corresponding ORs are <em>not</em> equal to 1).</p>
<p>The output of <code>summary()</code> also provides information on the <code>Deviance Residuals</code> (top of the output), <code>Null</code> and <code>Residual Deviance</code>, and <code>AIC</code>. (bottom). The meaning of these is explained in the next section.</p>
</div>
</div>
<div id="assessment-of-fit" class="section level2 hasAnchor" number="2.5">
<h2><span class="header-section-number">2.5</span> Assessment of fit<a href="the-logistic-regression-model-lrm.html#assessment-of-fit" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="deviance-likelihood-ratio-test-and-aic" class="section level3 hasAnchor" number="2.5.1">
<h3><span class="header-section-number">2.5.1</span> Deviance, likelihood ratio test and AIC<a href="the-logistic-regression-model-lrm.html#deviance-likelihood-ratio-test-and-aic" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The <em>deviance</em> is a measure used to asses the fit of models estimated using maximum likelihood. However, unlike <span class="math inline">\(R^2\)</span> in GLMs, the deviance is not a measure of <em>goodness of fit</em>, but rather it is a measure of <em>lack of fit</em>: the higher the deviance, the worse the fit; and the lower the deviance, the better the fit.</p>
<p>The summary of model <code>m_tf</code> we got in the previous section showed two deviance values:</p>
<ul>
<li><p><code>Null deviance</code>: the deviance of the so called <em>null model</em>, a model including only the intercept term, and therefore predicting the outcome as a constant. This was 845.42 on 999 degrees of freedom. The degrees of freedom is the difference between the samples size and the number of parameters estimated in a model. Because the sample size is 1000 and the null model has only one parameter (intercept), the degrees of freedom for the null model are 1000 - 1 = 999.</p></li>
<li><p><code>Residual deviance</code>: the deviance of model <code>m_tf</code> includig <code>tbsa</code>. This was 516.68 on 997 degrees of freedom (1000 - 3 estimated parameters).</p></li>
</ul>
<p>By including <code>tbsa</code> and <code>flame</code> as predictors, the deviance was reduced from 845.42 on 999 degrees of freedom (null model) to 516.68 on 997 degrees of freedom (current model). The change in degrees of freedom reflects the fact that we added two parameters to the null model: the coefficients for <code>tbsa</code> and <code>flameYes</code>; and this resulted in a very important reduction of the deviance, implying a much better fit.</p>
<p>We can judge the significance of the reduction in the deviance of model <code>m_tf</code> using the <code>anova()</code> function with the argument <code>test = "LRT"</code> (for a <em>Likelihood Ratio Test</em>):</p>
<div class="sourceCode" id="cb102"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb102-1"><a href="the-logistic-regression-model-lrm.html#cb102-1" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(m_tf, <span class="at">test =</span> <span class="st">&quot;LRT&quot;</span>)</span></code></pre></div>
<pre><code>Analysis of Deviance Table

Model: binomial, link: logit

Response: outcome == &quot;Dead&quot;

Terms added sequentially (first to last)

      Df Deviance Resid. Df Resid. Dev  Pr(&gt;Chi)    
NULL                    999     845.42              
tbsa   1  306.763       998     538.65 &lt; 2.2e-16 ***
flame  1   21.978       997     516.68 2.758e-06 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>The output shows how the residual deviance changes as we add parameters to the null model. The residual deviance for the null model is 845.42 on 999 degrees of freedom (first row). By adding one coefficient for <code>tbsa</code> (second row), the residual deviance drops to 538.65, which is a drop of 845.42 - 538.65 = 306.77, and this deviance reduction is statistically significant according to the Likelihood Ratio Test (LRT) (<code>Pr(&gt;Chi)</code>), which means a significant improvement in fit. By adding one addition coefficient for <code>flame</code> (third row), the residual deviance drops to 516.68, for a change of 21.98), which again is a significant improvement in the fit. The residual deviance for the null model and model <code>m_tf</code> (last row of the output) are exactly the same values we saw at the bottom of the results provided by <code>summary(m_tf)</code>, but here we can see that model <code>m_tf</code> significantly improves the fit compared to the null model, according to the LRT.</p>
<p>Last, the <code>AIC</code> value appearing at the end of the <code>summary()</code> output, is a measure called <em>Akaike information criterion</em> (AIC). This is the deviance penalized for model complexity. In fact it is computed by adding twice the number of parameters to the deviance. Therefore, as it is the case with the deviance, the lower the AIC, the better the fit.</p>
<p>The <code>Deviance residuals</code> at the top of the <code>summary()</code> output shows a brief descriptive analysis of such residuals. Deviance residuals are scaled components of the deviance, so that they provide a measure of how each observation contributes to the deviance. When a model is correct, deviance residuals follow a standard normal distribution, and therefore we expect them to have absolute values below 3. In the case of model <code>m_tf</code>, all deviance residuals are lower than 3 in abolute value.</p>
</div>
<div id="hosmer-lemeshow-test" class="section level3 hasAnchor" number="2.5.2">
<h3><span class="header-section-number">2.5.2</span> Hosmer-Lemeshow test<a href="the-logistic-regression-model-lrm.html#hosmer-lemeshow-test" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A quite popular way to assess the goodness of fit (GOF) of a LRM is the <a href="https://en.wikipedia.org/wiki/Hosmer%E2%80%93Lemeshow_test">Hosmer-Lemeshow GOF test</a>. In this test, observations are grouped according to the deciles of the predicted values, and expected frequencies are computed in a contingency table defined by the decile group and the observed outcome: the model probabilities are summed for cases with the event, and the complementary of model probabilities are sumed for cases without the event). Then, a goodness of fit chi-square test is used to compare expected and observed counts in this contingency table. Therefore, a low <em>p</em> value in this test is evidence of a bad fit.</p>
<p>Package <code>ResourceSelection</code> has a function <code>hoslem.test()</code> implementing the Hosmer-Lemeshow GOF test. The first argument to this function is the vector of observed outcomes coded as an event indicator (0 for no event, 1 for event), or a logical vector (with TRUE for events and FALSE for non-events). The second argument is the vector of model probabilities obtaned with <code>predict()</code>.</p>
<div class="sourceCode" id="cb104"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb104-1"><a href="the-logistic-regression-model-lrm.html#cb104-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ResourceSelection)</span>
<span id="cb104-2"><a href="the-logistic-regression-model-lrm.html#cb104-2" aria-hidden="true" tabindex="-1"></a><span class="fu">hoslem.test</span>(d<span class="sc">$</span>outcome <span class="sc">==</span> <span class="st">&quot;Dead&quot;</span>, <span class="fu">predict</span>(m_tf, <span class="at">type=</span><span class="st">&quot;response&quot;</span>))</span></code></pre></div>
<pre><code>
    Hosmer and Lemeshow goodness of fit (GOF) test

data:  d$outcome == &quot;Dead&quot;, predict(m_tf, type = &quot;response&quot;)
X-squared = 7.9975, df = 8, p-value = 0.4337</code></pre>
<p>In this case, the <em>p</em> value from the test does not reject the null hypothesis that model <code>m_tf</code> is correct at the usual 0.05 <span class="math inline">\(\alpha\)</span>-level.</p>
</div>
<div id="other-measures-of-fit" class="section level3 hasAnchor" number="2.5.3">
<h3><span class="header-section-number">2.5.3</span> Other measures of fit<a href="the-logistic-regression-model-lrm.html#other-measures-of-fit" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Goodness of fit measures in the vein of <span class="math inline">\(R^2\)</span> for GLMs are also available for LRMs. One such measure is the McFadden’s <span class="math inline">\(pseudo-R^2\)</span>, but these exceed the ambitions of this book (for the interested reader, a link to a blog post on this measure is provided in the resources section at the end of this chapter).</p>
</div>
</div>
<div id="model-predictions-and-classification" class="section level2 hasAnchor" number="2.6">
<h2><span class="header-section-number">2.6</span> Model predictions and classification<a href="the-logistic-regression-model-lrm.html#model-predictions-and-classification" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Logistic regression analyses are often is done for predictive purposes. This is the case of prognostic models, such as the burns example: it would be interesting to predict the outcome when a patient with burns is admitted to the hospital. In this cases, a more focused way to assess the utility of a LRM is by looking at how well the model classifies the observations. Let’s see how.</p>
<div id="predictions-and-confusion-matrix" class="section level3 hasAnchor" number="2.6.1">
<h3><span class="header-section-number">2.6.1</span> Predictions and confusion matrix<a href="the-logistic-regression-model-lrm.html#predictions-and-confusion-matrix" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>First we need to compute the model predictions with <code>predict()</code>. However, by default, <code>predict()</code> wil provide log-odds (probabilities in the logit scale). If we want to get the probabilities themselves, we need to use the argument type = “response”:</p>
<div class="sourceCode" id="cb106"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb106-1"><a href="the-logistic-regression-model-lrm.html#cb106-1" aria-hidden="true" tabindex="-1"></a>predicted_prob <span class="ot">&lt;-</span> <span class="fu">predict</span>(<span class="st">`</span><span class="at">m_tf</span><span class="st">`</span>, <span class="at">type =</span> <span class="st">&quot;response&quot;</span>) </span>
<span id="cb106-2"><a href="the-logistic-regression-model-lrm.html#cb106-2" aria-hidden="true" tabindex="-1"></a>predicted_prob <span class="sc">%&gt;%</span> <span class="fu">head</span>()</span></code></pre></div>
<pre><code>         1          2          3          4          5          6 
0.29686890 0.02377125 0.01889882 0.01889882 0.08549355 0.02767979 </code></pre>
<p>Then we need to use this probabilities to guess if a case has presented, or not present, the outcome. A reasonable decision is to assume the outcome is present if the probability is higher than 0.5. So, we can compute this guess with an <code>ifelse()</code> statement. Last, we crosstabulate the guess with the actual value of the outcome variable <code>outcome</code>:</p>
<div class="sourceCode" id="cb108"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb108-1"><a href="the-logistic-regression-model-lrm.html#cb108-1" aria-hidden="true" tabindex="-1"></a>guess <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(predicted_prob <span class="sc">&gt;</span> <span class="fl">0.5</span>, <span class="st">&quot;Dead&quot;</span>, <span class="st">&quot;Alive&quot;</span>)</span>
<span id="cb108-2"><a href="the-logistic-regression-model-lrm.html#cb108-2" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(guess, d<span class="sc">$</span>outcome)</span></code></pre></div>
<pre><code>       
guess   Alive Dead
  Alive   837   79
  Dead     13   71</code></pre>
<p>This cross tabulation of cases according to their actual value of the outcome variable and the model-based prediction is sometimes called a <em>confusion matrix</em>. Different measures can be derived from the confusion matrix, such as:</p>
<ul>
<li><p><em>Classification accuracy</em>, which is the proportion of correct predictions: (837 + 71) / 1000 = 0.908 or 90.8%.</p></li>
<li><p><em>Sensitivity</em>, which the proportion of deaths that are correctly classified: 71 / (71 + 79) = 0.473 or 47.4%.</p></li>
<li><p><em>Specificity</em>, which the proportion of survivors that are correctly classified: 837 / (837 + 13) = 0.985 or 98.5%.</p></li>
<li><p><em>Youden index</em> = sensitivity + specificity - 1; 0.908 + 0.473 - 1 = 0.381.</p></li>
</ul>
<p>Sensitivity and specificity are the main measures of the accuracy of a diagnostic or prognostic marker. The higher they are, the better the marker will be able to discriminate the two states it is supposed to distinguish. However, a problem with them is that the confusion matrix from which they are computed depends on the cutpoint used to dichotomize the probabilities predicted by the model.</p>
<p>Figure <a href="the-logistic-regression-model-lrm.html#fig:2-thres">2.2</a> shows the distributions of predicted probabilities by outcome, and the threshold used on these probabilities to guess the outcome (dashed line). The confusion matrix is nothing but counting how many cases there are on each side of the dashed line, for both outcomes (Alive or Death).</p>
<div class="sourceCode" id="cb110"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb110-1"><a href="the-logistic-regression-model-lrm.html#cb110-1" aria-hidden="true" tabindex="-1"></a><span class="fu">gf_jitter</span>(predicted_prob <span class="sc">~</span> outcome, <span class="at">data =</span> d, <span class="at">alpha =</span> <span class="fl">0.3</span>, </span>
<span id="cb110-2"><a href="the-logistic-regression-model-lrm.html#cb110-2" aria-hidden="true" tabindex="-1"></a>          <span class="at">color =</span> <span class="sc">~</span><span class="fu">fct_rev</span>(outcome), <span class="at">show.legend =</span> <span class="cn">FALSE</span>)  <span class="sc">%&gt;%</span> </span>
<span id="cb110-3"><a href="the-logistic-regression-model-lrm.html#cb110-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">gf_hline</span>(<span class="at">yintercept=</span><span class="fl">0.5</span>, <span class="at">linetype =</span> <span class="dv">2</span>) <span class="sc">+</span> <span class="fu">coord_flip</span>()</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:2-thres"></span>
<img src="_main_files/figure-html/2-thres-1.png" alt="Model 1 predicted probabilities by outcome" width="672" />
<p class="caption">
Figure 2.2: Model 1 predicted probabilities by outcome
</p>
</div>
<p><br />
</p>
<p>Although the 0.5 threshold we chose to guess the outcome may look reasonable, it is anyway arbitrary. The problem is then, how would the sensitivity and specificity change if we chose a different threshold? The solution to this question in the next section.</p>
</div>
<div id="roc-curves-and-auc" class="section level3 hasAnchor" number="2.6.2">
<h3><span class="header-section-number">2.6.2</span> ROC curves and AUC<a href="the-logistic-regression-model-lrm.html#roc-curves-and-auc" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Receiver operating characteristic (ROC) curves are a common way to asses the accuracy of a diagnostic or prognostic method that produces a quantitative result. A ROC curve is a graphical representation of the sensitivity and specificity values we get by changing the threshold to guess a death.</p>
<p>Package <code>pROC</code> allows to plot ROC curves. The <code>roc()</code> function in this package computes a <em>roc object</em>, that can be plotted with the base R function <code>plot()</code>. We have used the optional argument <code>print.thres</code> to mark the “best” threshold, that is, the one that maximizes the <em>Youden index</em> (the sum of sensitivity and specificity minus one):</p>
<div class="sourceCode" id="cb111"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb111-1"><a href="the-logistic-regression-model-lrm.html#cb111-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(pROC)</span>
<span id="cb111-2"><a href="the-logistic-regression-model-lrm.html#cb111-2" aria-hidden="true" tabindex="-1"></a>m_tf_roc <span class="ot">&lt;-</span> <span class="fu">roc</span>(d<span class="sc">$</span>outcome <span class="sc">~</span> predicted_prob)</span>
<span id="cb111-3"><a href="the-logistic-regression-model-lrm.html#cb111-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(m_tf_roc, <span class="at">print.thres =</span> <span class="st">&quot;best&quot;</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:2-roc"></span>
<img src="_main_files/figure-html/2-roc-1.png" alt="ROC curve for model `m_tf`" width="384" />
<p class="caption">
Figure 2.3: ROC curve for model <code>m_tf</code>
</p>
</div>
<p><br />
</p>
<p>Figure <a href="the-logistic-regression-model-lrm.html#fig:2-roc">2.3</a> shows the ROC curve for the probabilities computed from model <code>m_tf</code>. This was built by computing the sensitivity and the complementary of specificity (1-specificity) resulting from each possible threshold; each threshold contributes a point to the ROC curve, which results from joining the points. The point marked in the ROC curve is the one that maximizes the Youden index (and therefore the sum of senitivity and specificity), and corresponds to the threshold 0.158, which results in a specificity of 0.879 and a sensitivity is 0.773. We can easily verify this by computing the confusion matrix for this threshold:</p>
<div class="sourceCode" id="cb112"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb112-1"><a href="the-logistic-regression-model-lrm.html#cb112-1" aria-hidden="true" tabindex="-1"></a>guess <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(predicted_prob <span class="sc">&gt;=</span> <span class="fl">0.158</span>, <span class="st">&quot;Dead&quot;</span>, <span class="st">&quot;Alive&quot;</span>)</span>
<span id="cb112-2"><a href="the-logistic-regression-model-lrm.html#cb112-2" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(guess, d<span class="sc">$</span>outcome)</span></code></pre></div>
<pre><code>       
guess   Alive Dead
  Alive   747   34
  Dead    103  116</code></pre>
<p>From this confusion matrix, the specificity is 747 / (747+103) = 0.879, and the sensitivity is 116 / (116 + 34) = 0.773.</p>
<p>In general, a ROC curve closely approaching the upper left corner (which corresponds to a sensitivity and specificity of 1) reflects a good discrimination ability. Conversely, a ROC curve very close to the diagonal indicates very poor discrimination ability. The <em>area under the curve</em> (AUC) is a measure frequently used to characterize the discrimination ability of a marker, since it will be high when a curves approaches the upper left corner, and low when it is close to the diagonal. AUC values can range from 0 to 1, and the higher its value the better the discrimination ability of a marker.</p>
<p>The AUC of the ROC curve of figure <a href="the-logistic-regression-model-lrm.html#fig:2-roc">2.3</a> can be easily computed with function <code>auc()</code> on the roc object <code>m_tf_roc</code>:</p>
<div class="sourceCode" id="cb114"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb114-1"><a href="the-logistic-regression-model-lrm.html#cb114-1" aria-hidden="true" tabindex="-1"></a><span class="fu">auc</span>(m_tf_roc)</span></code></pre></div>
<pre><code>Area under the curve: 0.8877</code></pre>
<p>A 95% CI for the AUC can be obtained with function <code>ci.auc()</code>:</p>
<div class="sourceCode" id="cb116"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb116-1"><a href="the-logistic-regression-model-lrm.html#cb116-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ci.auc</span>(m_tf_roc)</span></code></pre></div>
<pre><code>95% CI: 0.8552-0.9202 (DeLong)</code></pre>
<p>In this case, the “marker” is the probability computed from model <code>m_tf</code>, and an AUC value of 0.8877 is quite high. This indicates that model <code>m_tf</code> has a reasonably good prognostic ability.</p>
</div>
</div>
<div id="comparing-models-1" class="section level2 hasAnchor" number="2.7">
<h2><span class="header-section-number">2.7</span> Comparing models<a href="the-logistic-regression-model-lrm.html#comparing-models-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Competing models can be compared according to how good is the fit, or according to their discrimination ability.</p>
<p>Let’s fit a second LRM including <code>age</code> as an additional predictor and get the summary:</p>
<div class="sourceCode" id="cb118"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb118-1"><a href="the-logistic-regression-model-lrm.html#cb118-1" aria-hidden="true" tabindex="-1"></a>m_tfa <span class="ot">&lt;-</span> <span class="fu">glm</span>(outcome <span class="sc">==</span> <span class="st">&quot;Dead&quot;</span> <span class="sc">~</span> tbsa <span class="sc">+</span> flame <span class="sc">+</span> age, <span class="at">family =</span> binomial, <span class="at">data =</span> d)</span>
<span id="cb118-2"><a href="the-logistic-regression-model-lrm.html#cb118-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(m_tfa)</span></code></pre></div>
<pre><code>
Call:
glm(formula = outcome == &quot;Dead&quot; ~ tbsa + flame + age, family = binomial, 
    data = d)

Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-2.86594  -0.27452  -0.10350  -0.03659   2.79049  

Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -7.891840   0.620523 -12.718   &lt;2e-16 ***
tbsa         0.096450   0.008719  11.062   &lt;2e-16 ***
flameYes     0.744372   0.336474   2.212   0.0269 *  
age          0.077424   0.007911   9.787   &lt;2e-16 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 845.42  on 999  degrees of freedom
Residual deviance: 357.79  on 996  degrees of freedom
AIC: 365.79

Number of Fisher Scoring iterations: 7</code></pre>
<p>The result of the Wald test for <code>age</code> suggests it is related to the outcome. But let’s see if the inclusion of <code>age</code> in the model results in a better fit, according to the LRT:</p>
<div class="sourceCode" id="cb120"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb120-1"><a href="the-logistic-regression-model-lrm.html#cb120-1" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(m_tf, m_tfa, <span class="at">test =</span> <span class="st">&quot;LRT&quot;</span>)</span></code></pre></div>
<pre><code>Analysis of Deviance Table

Model 1: outcome == &quot;Dead&quot; ~ tbsa + flame
Model 2: outcome == &quot;Dead&quot; ~ tbsa + flame + age
  Resid. Df Resid. Dev Df Deviance  Pr(&gt;Chi)    
1       997     516.68                          
2       996     357.79  1   158.89 &lt; 2.2e-16 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>We see that by adding a coefficient for <code>age</code> the deviance has dropped by 158.89, wich on a single degree of freedom is highly significant. Therefore, adding <code>age</code> to the model results in a significant improvement in the fit.</p>
<p>To see if the improvement in the fit is worth the greater complexity of the model, we can compare the AIC of both models with function <code>AIC()</code>:</p>
<div class="sourceCode" id="cb122"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb122-1"><a href="the-logistic-regression-model-lrm.html#cb122-1" aria-hidden="true" tabindex="-1"></a><span class="fu">AIC</span>(m_tf, m_tfa) </span></code></pre></div>
<pre><code>      df      AIC
m_tf   3 522.6766
m_tfa  4 365.7903</code></pre>
<p>Because the AIC is much lower for model <code>m_tfa</code>, this model is preferable.</p>
<p>Now let’s look at the discrimination ability of both models, by plotting their ROC curves. We need to compute the roc object for this model, and then plot it along with the roc object computed previously for model <code>m_tf</code>. Note the argument <code>add = TRUE</code> in the second call to <code>plot()</code>, to overlay this curve to the previous one (instead of producing a different graphic):</p>
<div class="sourceCode" id="cb124"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb124-1"><a href="the-logistic-regression-model-lrm.html#cb124-1" aria-hidden="true" tabindex="-1"></a>m_tfa_roc <span class="ot">&lt;-</span> <span class="fu">roc</span>(d<span class="sc">$</span>outcome <span class="sc">~</span> <span class="fu">predict</span>(<span class="st">`</span><span class="at">m_tfa</span><span class="st">`</span>, <span class="at">type =</span> <span class="st">&quot;response&quot;</span>) )</span>
<span id="cb124-2"><a href="the-logistic-regression-model-lrm.html#cb124-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(m_tf_roc, <span class="at">col =</span> <span class="st">&quot;steelblue&quot;</span>)</span>
<span id="cb124-3"><a href="the-logistic-regression-model-lrm.html#cb124-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(m_tfa_roc, <span class="at">col =</span> <span class="st">&quot;darkred&quot;</span>, <span class="at">add =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:2-roc-comp"></span>
<img src="_main_files/figure-html/2-roc-comp-1.png" alt="ROC curve for models `m_tf` (blue) and `m_tfa` (red)" width="384" />
<p class="caption">
Figure 2.4: ROC curve for models <code>m_tf</code> (blue) and <code>m_tfa</code> (red)
</p>
</div>
<p><br />
</p>
<p>Looking at the figure, it is clear that model <code>m_tfa</code> has a better discrimination ability than model <code>m_tf</code>. Let`s compute the AUCs for these models:</p>
<div class="sourceCode" id="cb125"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb125-1"><a href="the-logistic-regression-model-lrm.html#cb125-1" aria-hidden="true" tabindex="-1"></a><span class="fu">auc</span>(m_tf_roc)</span></code></pre></div>
<pre><code>Area under the curve: 0.8877</code></pre>
<div class="sourceCode" id="cb127"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb127-1"><a href="the-logistic-regression-model-lrm.html#cb127-1" aria-hidden="true" tabindex="-1"></a><span class="fu">auc</span>(m_tfa_roc)</span></code></pre></div>
<pre><code>Area under the curve: 0.9581</code></pre>
<p>We see that model <code>m_tfa</code> has a higher AUC than model <code>m_tf</code>. Now, we could test for the significance of the difference in AUCs with function <code>roc.test()</code>, passing it as arguments the two roc objects to be compared :</p>
<div class="sourceCode" id="cb129"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb129-1"><a href="the-logistic-regression-model-lrm.html#cb129-1" aria-hidden="true" tabindex="-1"></a><span class="fu">roc.test</span>(m_tf_roc, m_tfa_roc)</span></code></pre></div>
<pre><code>
    DeLong&#39;s test for two correlated ROC curves

data:  m_tf_roc and m_tfa_roc
Z = -4.8281, p-value = 1.379e-06
alternative hypothesis: true difference in AUC is not equal to 0
95 percent confidence interval:
 -0.09901206 -0.04183500
sample estimates:
AUC of roc1 AUC of roc2 
  0.8877176   0.9581412 </code></pre>
<p>The output shows a very low <em>p</em> value, which provides evidence of a statistically significant difference in AUC. Therefore, we can conclude that model <code>m_tfa</code> has a better discrimination ability than model <code>m_tf</code>. The output also shows the 95% CI for the <em>difference</em> in AUC. Therefore, from this output we can conclude that the AUC of model <code>m_tfa</code> is 0.042 to 0.099 higher than that of model <code>m_tf</code>.</p>
<p>The previous test (known as DeLong’s test) assumes a normal distribution of the marker in both outcome groups. If the assumption is not reasonable, a bootstrapped test can be produced with argument <code>method = "bootstrap"</code> (see <code>?roc.test</code>). In this case this would be more appropriate since the distribution of the model predicted probabilities is quite assymetric (see figure <a href="the-logistic-regression-model-lrm.html#fig:2-thres">2.2</a>), but the result is similar (result not shown) due to the robustness of DeLong’s test when the sample size is as high as it is in this case.</p>
<p>In summary, model <code>m_tfa</code> is superior to model <code>m_tf</code> in terms of both fitting the data and discriminating the two possible outcomes (dead or alive).</p>
</div>
<div id="confounding-1" class="section level2 hasAnchor" number="2.8">
<h2><span class="header-section-number">2.8</span> Confounding<a href="the-logistic-regression-model-lrm.html#confounding-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>LRM can be used to assess confounding, and to get ORs <em>adjusted</em> for confounders. Let’s fit a univariate LRM to asses the effect of inhalation injury on the outcome, and get the exponentials of the model coefficients:</p>
<div class="sourceCode" id="cb131"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb131-1"><a href="the-logistic-regression-model-lrm.html#cb131-1" aria-hidden="true" tabindex="-1"></a>m_i <span class="ot">&lt;-</span> <span class="fu">glm</span>(outcome <span class="sc">==</span> <span class="st">&quot;Dead&quot;</span> <span class="sc">~</span> inh_inj, <span class="at">family =</span> binomial, <span class="at">data =</span> d)</span>
<span id="cb131-2"><a href="the-logistic-regression-model-lrm.html#cb131-2" aria-hidden="true" tabindex="-1"></a><span class="fu">exp</span>(<span class="fu">coef</span>(m_i))</span></code></pre></div>
<pre><code>(Intercept)  inh_injYes 
    0.09750    14.76923 </code></pre>
<p>From the output we see that the OR for <code>inh_injYes</code> is 14.77, implying that the odds of dead is more than fourteen times higher when there is inhalation injury than when there is not. However, <code>inh_inj</code> and <code>tbsa</code> are highly related, as can be seen in figure <a href="the-logistic-regression-model-lrm.html#fig:2-confounding">2.5</a>:</p>
<div class="sourceCode" id="cb133"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb133-1"><a href="the-logistic-regression-model-lrm.html#cb133-1" aria-hidden="true" tabindex="-1"></a><span class="fu">gf_boxplot</span>(tbsa <span class="sc">~</span> inh_inj, <span class="at">data =</span> d) <span class="sc">+</span> <span class="fu">coord_flip</span>()</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:2-confounding"></span>
<img src="_main_files/figure-html/2-confounding-1.png" alt="Boxplot of `tbsa` by `inh_inj`" width="672" />
<p class="caption">
Figure 2.5: Boxplot of <code>tbsa</code> by <code>inh_inj</code>
</p>
</div>
<p><br />
</p>
<p>This may suppose that, when we look at the association of <code>inh_inj</code> and the outcome without taking <code>tbsa</code> into account, the effect of <code>tbsa</code> and that of <code>inh_inj</code> are confounded. To see if this is the case, let’s fit a model with both <code>tbsa</code> and <code>inh_inj</code>, and estimate the OR from this model:</p>
<div class="sourceCode" id="cb134"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb134-1"><a href="the-logistic-regression-model-lrm.html#cb134-1" aria-hidden="true" tabindex="-1"></a>m_ti <span class="ot">&lt;-</span> <span class="fu">glm</span>(outcome <span class="sc">==</span> <span class="st">&quot;Dead&quot;</span> <span class="sc">~</span> tbsa <span class="sc">+</span> inh_inj, <span class="at">family =</span> binomial, <span class="at">data =</span> d)</span>
<span id="cb134-2"><a href="the-logistic-regression-model-lrm.html#cb134-2" aria-hidden="true" tabindex="-1"></a><span class="fu">exp</span>(<span class="fu">coef</span>(m_ti))</span></code></pre></div>
<pre><code>(Intercept)        tbsa  inh_injYes 
 0.03406189  1.07563276  3.63233807 </code></pre>
<p>Acording to this model, the OR for the inhalation injury is 3.63. This OR is <em>adjusted for</em> <code>tbsa</code>, and is much lower than 14.77. So, this is a case where <code>tbsa</code> acted as a confounder when we estimated the effect of inhalation injuries in the univariate LRM. The term <em>adjusted OR</em>, which is quite common in the medical research literature, means that the OR has been estimated from models that include potential confounders.</p>
</div>
<div id="interaction-effect-modification-1" class="section level2 hasAnchor" number="2.9">
<h2><span class="header-section-number">2.9</span> Interaction (effect modification)<a href="the-logistic-regression-model-lrm.html#interaction-effect-modification-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We could think that the effect of flames ca be different depending on the age of patients. To test this, we can fit a model with an additional term representing the interaction of <code>flame</code> and <code>age</code>. In function <code>glm()</code>, interaction terms are specified just a in function <code>lm()</code> for GLMs. Here we use the asterisk to specify an interaction between <code>flame</code> and <code>age</code>:</p>
<div class="sourceCode" id="cb136"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb136-1"><a href="the-logistic-regression-model-lrm.html#cb136-1" aria-hidden="true" tabindex="-1"></a>m_tfa_int <span class="ot">&lt;-</span> <span class="fu">glm</span>(outcome <span class="sc">==</span> <span class="st">&quot;Dead&quot;</span> <span class="sc">~</span> tbsa <span class="sc">+</span> flame <span class="sc">*</span> age, <span class="at">family =</span> binomial, <span class="at">data =</span> d)</span></code></pre></div>
<p>Let`s get a summary of this model:</p>
<div class="sourceCode" id="cb137"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb137-1"><a href="the-logistic-regression-model-lrm.html#cb137-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(m_tfa_int)</span></code></pre></div>
<pre><code>
Call:
glm(formula = outcome == &quot;Dead&quot; ~ tbsa + flame * age, family = binomial, 
    data = d)

Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-2.84247  -0.27258  -0.09219  -0.01056   2.85182  

Coefficients:
               Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  -10.482093   1.273295  -8.232  &lt; 2e-16 ***
tbsa           0.098108   0.008993  10.909  &lt; 2e-16 ***
flameYes       3.816893   1.234002   3.093  0.00198 ** 
age            0.118519   0.018188   6.516 7.21e-11 ***
flameYes:age  -0.050326   0.018760  -2.683  0.00731 ** 
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 845.42  on 999  degrees of freedom
Residual deviance: 349.76  on 995  degrees of freedom
AIC: 359.76

Number of Fisher Scoring iterations: 8</code></pre>
<p><br />
</p>
<p>The result of the Wald test for the interaction term <code>flameYes:age</code> provides some evidence of an interaction between flames and age (<em>p</em> = 0.007). Another (possibly better) way to judge the existence of an interaction is to compare the fits of this model and the model <em>without</em> the interaction term by a LRT:</p>
<div class="sourceCode" id="cb139"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb139-1"><a href="the-logistic-regression-model-lrm.html#cb139-1" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(m_tfa, m_tfa_int, <span class="at">test =</span> <span class="st">&quot;LRT&quot;</span>)</span></code></pre></div>
<pre><code>Analysis of Deviance Table

Model 1: outcome == &quot;Dead&quot; ~ tbsa + flame + age
Model 2: outcome == &quot;Dead&quot; ~ tbsa + flame * age
  Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)   
1       996     357.79                        
2       995     349.76  1   8.0272 0.004608 **
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>From this output, we see that the addition of the interaction term results in a drop of 8.0272 in the residual deviance, which on one degree of freedom (for one additional parameter) is statistically significant according to the LRT (<em>p</em> = 0.004).</p>
<p>Last, we can look at the AIC of both models:</p>
<div class="sourceCode" id="cb141"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb141-1"><a href="the-logistic-regression-model-lrm.html#cb141-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(m_tfa)<span class="sc">$</span>aic</span></code></pre></div>
<pre><code>[1] 365.7903</code></pre>
<div class="sourceCode" id="cb143"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb143-1"><a href="the-logistic-regression-model-lrm.html#cb143-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(m_tfa_int)<span class="sc">$</span>aic</span></code></pre></div>
<pre><code>[1] 359.7631</code></pre>
<p>The lower AIC of the model with interaction indicates a better fit of this model. So, according to all three criteria (Wald test, LRT and AIC), the model containing an interaction between age and flames provides a better representation of the data.</p>
<p>Because an interaction between two predictors means that the effect of one of them depends on the value of the other, this effect cannot be expressed as a single OR. For instance, from model <code>m_tfa_int</code> we cannot give a single OR to characterize the effect of flames model because this effect depends on age.</p>
<p>In general, when a LRM contains an interaction term of a risk factor (say, <span class="math inline">\(X_1\)</span>) and an effect modifier (<span class="math inline">\(X_2\)</span>), we may want to compute the OR comparing two values of the risk factor (say, <span class="math inline">\(X_1 = k_1\)</span>, vs <span class="math inline">\(X_1 = k_2\)</span>) for different values of the effect modifier <span class="math inline">\(X_2\)</span>. This can be done looking at the <em>log_odds</em> in such cases:</p>
<ul>
<li><p>When <span class="math inline">\(x_1 = k_1: \qquad log(\frac{\pi|X_1=k1}{1-\pi|X_1=k1}) = \beta_0 + \beta_1 \: k_1 + \beta_2 \: X_2+\beta_{12} \: k_1 \: X_2\)</span></p></li>
<li><p>When <span class="math inline">\(X_1 = k_2: \qquad log(\frac{\pi|X_1=k2}{1-\pi|X_1=k2}) = \beta_0 + \beta_1 \: k_2 + \beta_2 \: X_2+\beta_{12} \: k_2 \: X_2\)</span></p></li>
</ul>
<p><br />
</p>
<p>The difference between these two <em>log_odds</em> will be the log of the OR:</p>
<p><span class="math inline">\(log(\frac{\pi|X_1=k1}{1-\pi|X_1=k1})-log(\frac{\pi|X_1=k2}{1-\pi|X_1=k2}) \quad = \quad log(OR) \quad = \quad \beta_1 (k_1-k_2) + \beta_{12}(k_1-k_2) \: X_2\)</span></p>
<p><br />
</p>
<p>An therefore the OR comparing the odds (<span class="math inline">\(k_1\)</span> over <span class="math inline">\(k_2)\)</span> is:</p>
<p><span class="math display">\[OR = e^{\beta_1 \: (k_1-k_2) + \beta_{12} \: (k_1-k_2) \: X_2}\]</span></p>
<p>This equation shows how the value of the OR comparing two values of the risk factor (<span class="math inline">\(k_1\)</span> vs <span class="math inline">\(k_2\)</span>) depends on the value of the effect modifier <span class="math inline">\(X_2\)</span>.</p>
<p>The computation and interpretation of ORs estimated in models with interaction terms is not as straightforward as in the case of models with main effects only (i.e, without interaction terms). For a worked example, see <a href="https://www.youtube.com/watch?v=LX2HBvCKjBY">this video</a>.</p>
</div>
<div id="model-diagnostics" class="section level2 hasAnchor" number="2.10">
<h2><span class="header-section-number">2.10</span> Model diagnostics<a href="the-logistic-regression-model-lrm.html#model-diagnostics" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Unlike the GLM, the LRM does not assume normality of the residuals. In terms of distributional assumptions, the LRM only assumes that the outcome variable follows a binomial distribution with mean and variance that depend on the explanatory variables. For this reason, the analysis of residuals is not as easy and informative as in the case of GLMs.</p>
<p>The LRM implicitly assumes that the relation of continuous predictors is linear in the logit scale. To assess whether or not this assumption is reasonable, the fitted values can be plotted against each continuous predictor. Let’s do it for the <code>m_tfa_int</code> model:</p>
<div class="sourceCode" id="cb145"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb145-1"><a href="the-logistic-regression-model-lrm.html#cb145-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggformula)</span>
<span id="cb145-2"><a href="the-logistic-regression-model-lrm.html#cb145-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb145-3"><a href="the-logistic-regression-model-lrm.html#cb145-3" aria-hidden="true" tabindex="-1"></a>p1 <span class="ot">&lt;-</span> <span class="fu">gf_point</span>(<span class="fu">predict</span>(m_tfa_int) <span class="sc">~</span> tbsa, <span class="at">data =</span> d) <span class="sc">+</span></span>
<span id="cb145-4"><a href="the-logistic-regression-model-lrm.html#cb145-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;loess&quot;</span>)</span>
<span id="cb145-5"><a href="the-logistic-regression-model-lrm.html#cb145-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb145-6"><a href="the-logistic-regression-model-lrm.html#cb145-6" aria-hidden="true" tabindex="-1"></a>p2 <span class="ot">&lt;-</span> <span class="fu">gf_point</span>(<span class="fu">predict</span>(m_tfa_int) <span class="sc">~</span> age, <span class="at">data =</span> d) <span class="sc">+</span></span>
<span id="cb145-7"><a href="the-logistic-regression-model-lrm.html#cb145-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;loess&quot;</span>)</span>
<span id="cb145-8"><a href="the-logistic-regression-model-lrm.html#cb145-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb145-9"><a href="the-logistic-regression-model-lrm.html#cb145-9" aria-hidden="true" tabindex="-1"></a>p1 <span class="sc">+</span> p2</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-52-1.png" width="1056" style="display: block; margin: auto;" /></p>
<p>In this case, the linearity of the logit for both <code>tbsa</code> and <code>age</code> is not perfect, but it is not completely unreasonable. Maybe the model could be improved by using a second degree polynomial for each of these variables, but this would increase the model complexity, and might not necessarily improve the predictive capacity of the model; exploring it is left to the student as an optional exercise.</p>
<p>The detection of influential outliers can be easily made by looking at the plot of Cook’s distances that can be obtained as for GLMs:</p>
<div class="sourceCode" id="cb146"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb146-1"><a href="the-logistic-regression-model-lrm.html#cb146-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(m_tfa_int, <span class="dv">4</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-53-1.png" width="384" style="display: block; margin: auto;" /></p>
<p>In this case, all Cook’s distances are below 0.5, so there is no reason to worry about influential observations.</p>
</div>
<div id="resources-1" class="section level2 unnumbered hasAnchor">
<h2>Resources<a href="the-logistic-regression-model-lrm.html#resources-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li><p><a href="https://www.amazon.es/Applied-Logistic-Regression-Probability-Statistics/dp/0471356328">Applied Logistic Regression</a>, by D. Hosmer &amp; S. Lemeshow, is a very good book on LRM.</p></li>
<li><p>A very synthetic post in R-bloggers on <a href="https://www.r-bloggers.com/2015/08/evaluating-logistic-regression-models/">evaluating LRMs</a>.</p></li>
<li><p>McFadden’s <span class="math inline">\(pseudo-R^2\)</span> is a There are several <span class="math inline">\(R^2\)</span>-like measures for LRMs, such as <a href="http://thestatsgeek.com/2014/02/08/r-squared-in-logistic-regression/">McFadden’s <span class="math inline">\(pseudo-R^2\)</span></a>.</p></li>
<li><p>A shiny app to illustrate ROC curves can be found <a href="https://acobos.shinyapps.io/roc_curve/">here</a>: play with the threshold slider (and with the separation) to see what happens.</p></li>
<li><p>An R-bloggers post on how to fit a <a href="https://www.r-bloggers.com/2021/02/how-to-run-logistic-regression-on-aggregate-data-in-r/">LRM with aggregated (grouped) data</a>.</p></li>
<li><p>When there are many potential predictor variables, the number of possible models can be very large, and it is not feasible to fit and assess them one at a time. Read about model selection methods <a href="http://www.sthda.com/english/articles/36-classification-methods-essentials/150-stepwise-logistic-regression-essentials-in-r/">here</a> and <a href="https://xiaoruizhu.github.io/Data-Mining-R/lecture/4.B_LogisticReg_VS.html">here</a>.</p></li>
<li><p>A classical, yet interesting article on the important concept of <a href="https://pubmed.ncbi.nlm.nih.gov/10694730/">validation of prognostic models</a>; and a another, more recent article on the <a href="https://pubmed.ncbi.nlm.nih.gov/25981519/">validation of prediction models</a>.</p></li>
</ul>
</div>
<div id="exercises-1" class="section level2 unnumbered hasAnchor">
<h2>Exercises<a href="the-logistic-regression-model-lrm.html#exercises-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ol style="list-style-type: decimal">
<li><p>In section <a href="the-logistic-regression-model-lrm.html#logit-transformation">2.1</a> we stated that equations <a href="the-logistic-regression-model-lrm.html#eq:2-logistic">(2.1)</a> and <a href="the-logistic-regression-model-lrm.html#eq:2-logit">(2.2)</a> are equivalent. Can you prove it?</p></li>
<li><p>Suppose a logistic regression model includes a categorical variable <code>smoker</code> as predictor, and it is a factor with levels <code>never</code>, <code>past</code> and <code>current</code>.</p>
<ul>
<li>How many model coefficients refer to variable <code>smoke</code>? What do they correspond to?</li>
<li>What is the interpretation of the exponentials of these coefficient(s)?</li>
</ul></li>
<li><p>For each categorical variable (<code>gender</code>, <code>race</code>, <code>flame</code> and <code>inh_inj</code>), fit a univariate LRM, and:</p>
<ul>
<li>compute the exponential of the model coefficient of the predictor variable.</li>
<li>compute the contingency table of the predictor variable and <code>outcome</code>; from this table, compute the OR and compare it to the OR estimated from the LRM.</li>
</ul></li>
<li><p>Fit a LRM for <code>outcome == "Dead"</code> including all remaining variables in the dataset as predictors (call this model <code>m1</code>) and answer the following questions:</p>
<ul>
<li>According to the Wald test results, is there evidence of association with outcome for all predictors? What is the residual deviance of this model? What are the degrees of freedom and why? What is the AIC?</li>
</ul></li>
<li><p>Fit a simpler model by removing the predictors with no evidence of association to the outcome according to the Wald tests results (call this model <code>m2</code>):</p>
<ul>
<li>What are now the residual deviance, the degrees of freedom and the AIC?</li>
<li>Compare models <code>m1</code> and <code>m2</code> with a LRT and interpret the result.</li>
<li>Which of <code>m1</code> and <code>m2</code> is preferable according to the AIC?</li>
<li>Compute and interpret the ORs (and 95% CI) estimated from this model (give the OR for a 10 units increase of quantitative variables).</li>
</ul></li>
<li><p>The prognosis of many clinical conditions gets worse with age. It is therefore reasonable to think that age could be an effect modifier of <code>tbsa</code> or <code>inh_inj</code>. Check for a possible interaction of age with these two variables by adding appropriate interaction terms to model <code>m2</code> (and call this new model <code>m3</code>).</p>
<ul>
<li>What are now the residual deviance, the degrees of freedom and the AIC?</li>
<li>Compare models with a LRT and interpret the result.</li>
<li>Which of <code>m2</code> and <code>m3</code> is preferable according to the AIC?</li>
</ul></li>
<li><p>Plot the ROC curve of models <code>m1</code> <code>m2</code> and <code>m3</code>, and:</p>
<ul>
<li>compute the AUCs and corresponding 95% CIs using the bootstrap method (hint: look at the <code>method</code> argument in the help of function <code>ci.auc()</code>).</li>
<li>compare the AUCs with a) the DeLong’s test, and b) the bootstrap test. Are the <em>p</em> values of the two tests very different? Do they lead to different conclusions?</li>
<li>Which of the three models you think is preferable to be used as a prognostic tool?</li>
<li>For the model you think is preferable, determine the best threshold to maximize the Youden index, and produce the confusion matrix.</li>
</ul></li>
<li><p>How many different models (containing at least the intercept term and no interaction terms) could be fitted with the explanatory variables in the burns dataset?</p></li>
</ol>

</div>
</div>







<div class="footnotes">
<hr />
<ol start="2">
<li id="fn2"><p>If only the variable name is used in the left hand side of the formula, e.g., <code>glm(outcome ~ ...)</code>,then the <em>second</em> level of the factor <code>outcome</code> would be modeled. In this case, “Dead” is the second level of <code>outcome</code>, so that we could have attained the same result with <code>glm(outcome ~ tbsa + flame, family = binomial, data = d)</code>. However, it is safer to explicitly state the level whose probability we want to model.<a href="the-logistic-regression-model-lrm.html#fnref2" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="the-general-linear-model-glm.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
