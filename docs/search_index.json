[["index.html", "Statistical Modeling for Clinical Researchers Introduction", " Statistical Modeling for Clinical Researchers Albert Cobos Introduction This is an introductory book on statistical or probabilistic modeling for clinical researchers. There is a wide variety of statistical models, but this book covers only a few of them that are most frequently used in clinical research: the general linear model (GLM), the logistic regression model (LRM), and the Cox regression model (CRM). In many statistical models, statistical models express a variable (or a function of a variable) \\(Y\\) as a function of some other variables (\\(X_1, X_2, ..., X_k\\)). For instance, in the GML, a \\(Y\\) variable is expressed as a function of \\(X\\) variables plus an error term (\\(\\epsilon\\)): \\[Y = f(X_1, X_2, ..., X_k) + \\epsilon\\] Where, \\(Y\\) is called the dependent, predicted, explained or outcome variable. \\(X_1, X_2, ..., X_k\\) are called independent, predictor, or explanatory variables, and \\(\\epsilon\\) is a random variable with assumed to have a normal distribution. Because attempts to express a \\(Y\\) variable as a function of others are usually imperfect, the error term in the model represents the difference between the value of \\(Y\\) actually observed for an individual, and the prediction obtained from the explanatory variables (denoted by \\(\\hat{Y}\\)) : \\[\\epsilon = Y - f(X_1, X_2, ..., X_k) = Y - \\hat{Y}\\] Statistical modeling may be undertaken with two different goals, explanation or prediction, though in some cases both may be of interest. An example of an explanatory approach to modeling might be to understand why there is such a high variability in the number of daily births registered in the USA (in a particular year, this number ranged from about 7000 births to more than 10000 ), and what factors could explain it. In some cases, the relationships between the explanatory variables and the outcome variable are complex, and phenomena such as non-linearity, confounding or interactions can be explored via statistical models. An example of predictive modeling is the derivation of the Friedewald formula to estimate the blood concentration of LDL-cholesterol from the total cholesterol, HDL-cholesterol and triglycerides concentrations. Another example of predictive modeling is the determination of normal ranges for the forced expiratory volume (FEV, a spirometric measure of lung function) in healthy children, from gender, age and body height. The types of models we will cover differ mainly in the type of outcome variable (\\(Y\\)) we want to model. In a general linear model, the \\(Y\\) variable is continuous. The GLM includes as particular cases many classical analysis techniques, such as the t-test or the analysis of variance (in which the \\(X\\) variable(s) are categorical), the multiple linear regression analysys (in which \\(X\\) variables are continuous), and the analysis of covariance (in which some \\(X\\) variables are categorical and some are continuous). In the LRM the outcome variable is dichotomous, such as success or failure of a treatment. Last, in the CRM (also called Cox proportional hazards model) the outcome variable is the time until an event occurs, be it death or other. In all these types of models, the explanatory variables can be categorical, quantitative, or a mixture of both. Finally, a warning on terminology. The term generalized linear model is sometimes used to refer to a family of models that includes as particular cases the GLM, the LRM, and some other models, but not the CRM. Beware not to confuse generalized linear models with the general linear model, which is a particular case of the former. In many books on generalized linar models, the acronim GLM is used. However, in this book we use GLM as an acronim for the general linear model. "],["the-general-linear-model-glm.html", "1 The general linear model (GLM) 1.1 Data 1.2 Fitting a GLM 1.3 Inference on the model coefficients 1.4 Goodness-of-fit 1.5 Assessing assumptions 1.6 Probability intervals (or prediction intervals) 1.7 Models with two continuous predictors 1.8 Comparing models 1.9 Confounding 1.10 Interaction (effect modification) Resources Exercises", " 1 The general linear model (GLM) The general linear model (GLM) is a generalization of the simple linear regression model, allowing for more than one explanatory variable. In a GLM, a continuous outcome variable \\(Y\\) is expressed as a linear function of a set of explanatory or predictor variables \\(X\\), plus a random error term assumed to be normally distributed with constant variance \\(\\sigma^2\\): \\[\\begin{equation} Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_p X_p + \\epsilon \\qquad \\qquad \\epsilon \\sim N(0, \\sigma^2) \\tag{1.1} \\end{equation}\\] The deterministic part of the model (i.e., all terms but the random error term) provides predicted values (denoted as \\(\\hat{Y}\\)) for a given combination of \\(X\\) values, and these predictions are interpreted as the mean value of \\(Y\\) for all individuals with the given combination of \\(X\\) values: \\[\\hat{Y} = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_p X_p\\] So that the error terms in the model are just the difference between observed and predicted \\(Y\\) values: \\[\\epsilon = Y - \\hat{Y}\\] In the previous equations, the betas are the model coefficients or parameters, and our purpose will be to estimate these parameters, as well as the variance of the errors (\\(\\sigma^2\\)), from a sample of individuals in whom we have measured both the outcome variable \\(Y\\) and the explanatory variables \\(X_1, X_2, ..., X_p\\). Each of these coefficients is interpreted as the effect of the corresponding \\(X\\) variable on \\(Y\\), given all other \\(X\\) variables (i.e., when all other \\(X\\) variables are held constant). Explanatory variables in a GLM can be either quantitative or categorical. While there is no problem in entering a quantitative variable in the model equation, categorical variables cannot be entered as such. For instance, consider a model for variable height (cm) expressed as a function of variables age (years) and gender (male/female): \\[predicted\\quad height = \\beta_0 + \\beta_1 \\times age + \\beta_2 \\times gender\\] How are we supposed to compute the last term, if gender takes values \"male\" or \"female\"? To solve this problem, categorical variables, defined as factors in R, are used to create a set of dummy variables for each level of the factor but the first one. So, if gender has two levels only and the first level is \"male\", an indicator for \"female\" will be created (and named \\(genderfemale\\)), taking value 1 for females, and 0 for males. Then, the model actually fitted will be: \\[predicted\\quad height = \\beta_0 + \\beta_1 \\times age + \\beta_2 \\times genderfemale\\] Now, because \\(genderfemale\\) takes values 0 or 1, it is clear how to compute the predicted height for any individual. For instance, the predicted height for a 45 years female will be \\[\\beta_0 + \\beta_1 \\times 45 + \\beta_2 \\times 1 \\quad = \\quad \\beta_0 + \\beta_1 \\times 45 + \\beta_2\\], but for a 45 years male it will be \\[\\beta_0 + \\beta_1 \\times 45 + \\beta_2 \\times 0 \\quad = \\quad \\beta_0 + \\beta_1 \\times 45\\] In general, we will need \\(k-1\\) indicators to enter a factor with \\(k\\) levels in a model. For instance, suppose we want to include an explanatory factor race having levels \"white\", \"black\", \"asian\" and \"other\". In this case, we need three indicators, one for each of the levels but the first one (\"black\", \"asian\" and \"other\"), taking values 0 or 1 depending on race, as shown in table 1.1. Table 1.1: Indicators for race race black asian other white 0 0 0 black 1 0 0 asian 0 1 0 other 0 0 1 Thus, a model including the factor race would in fact have the following three terms to represent it (and possibly other explanatory variables): \\[\\beta_0 + \\beta_1 \\; raceblack + \\beta_2 \\; raceasian + \\beta_3 \\; raceother \\;+ \\; ... (other \\quad predictors)\\] In such a model, the predicted value for an individual depends on the value of race: when race is \"white\": \\(\\qquad \\beta_0 \\; + \\qquad \\; ... (other \\quad predictors)\\) when race is \"black\": \\(\\qquad\\beta_0 + \\beta_1 \\; + \\;... (other \\quad predictors)\\) when race is \"asian\": \\(\\qquad\\beta_0 + \\beta_2 \\; + \\;... (other \\quad predictors)\\) when race is \"other\": \\(\\qquad\\beta_0 + \\beta_3 \\; + \\;... (other \\quad predictors)\\) Fortunately, we do not need to compute these indicators to enter a categorical variable into a model. Provided the categorical variable is a factor, modeling functions such as lm() will compute them for us. In GLMs, the model parameters (i.e., the \\(\\beta\\) coefficients and the variance of the residuals \\(\\sigma^2\\)) are estimated so as to minimize the sum of the squares of the residuals. This criterion is known as least squares or ordinary least squares (OLS). 1.1 Data The lungcap dataset from package GLMsData (see ?lungcap) has data on the forced expiratory volume (a measure of lung capacity, expressed in litres) in 654 youth, as well as their gender and age, body height (in inches) and smoking habit. The following script gets the data, computes a new variable height (by converting inches to centimeters), defines factors for gender and smoke, saves the result as dataframe d, removes dataframe lungcap, and gets a summary of d: library(tidyverse) library(GLMsData) data(lungcap) # get the dataset d &lt;- lungcap %&gt;% janitor::clean_names() %&gt;% mutate(height = round(2.54 * ht), # converts from inches to cm gender = as.factor(gender), # creates factor for gender smoke = factor(smoke, # creates factor for smoke levels = 0:1, labels = c(&quot;no&quot;, &quot;yes&quot;))) %&gt;% select(-ht) # drops ht rm(lungcap) # removes lungcap summary(d) # summarizes d age fev gender smoke height Min. : 3.000 Min. :0.791 F:318 no :589 Min. :117.0 1st Qu.: 8.000 1st Qu.:1.981 M:336 yes: 65 1st Qu.:145.0 Median :10.000 Median :2.547 Median :156.0 Mean : 9.931 Mean :2.637 Mean :155.3 3rd Qu.:12.000 3rd Qu.:3.119 3rd Qu.:166.0 Max. :19.000 Max. :5.793 Max. :188.0 The summary shows that the age ranges from 3 to 19 years, males an females are about balanced in number, and there are few smokers. Suppose our goal is to establish normal ranges for the forced expiratory volume according to height and gender. Because smoking can impair the lung function, we want to exclude 65 smoking individuals from our analysis, so we subset dataframe d into dns (d non-smokers): dns &lt;- filter(d, smoke == &quot;no&quot;) Before undertaking any modelling exercise, let’s explore the relations of fev with height, age, and gender: library(ggformula) plot1 &lt;- gf_point(fev ~ age, data = dns, alpha=0.2) plot2 &lt;- gf_point(fev ~ height, data = dns, alpha=0.3) plot3 &lt;- gf_boxplot(fev ~ gender, data =dns) library(patchwork) plot1 + plot2 + plot3 Figure 1.1: Relations of FEV with age, height and gender in non-smokers In figure 1.1 (left), fev shows a quite strong, about linear relation with age , and even stronger with height (center), though a bit curvilinear. Also, the variability of fev seems to increase with both age and height. The distribution of fev values in men and women (right) are much overlapped, although in the former it extends to higher values. The non-linearity of the relation between fev and height, as well as the heteroscedasticity of fev with both age and height, are indications that the GLM model assumptions will not be met. But let’s start simple and fit a GLM anyway; later on we will see how to improve our analysis. 1.2 Fitting a GLM A GLM can be fitted with function lm(), by specifying the model in a formula as the first argument, and the dataframe in a second data= argument. The following script fits a model of fev based on height and gender, saves it as model1 and then prints it: model1 &lt;- lm(fev ~ height + gender, data = dns) model1 Call: lm(formula = fev ~ height + gender, data = dns) Coefficients: (Intercept) height genderM -5.33027 0.05093 0.10568 When we print a model object like model1, the estimates of the model Coefficients are shown. From these, the estimated regression equation is: predicted fev = -5.33027 + 0.05093 height + 0.10568 genderM The interpretation of the model coefficients is as follows: The intercept is the predicted fev for any observation having all explanatory variables equal to zero. In the example we are considering this interpretation is of no use, since zero is an impossible value for height. The coefficient for height means that, given the gender of an individual (or, for any gender), a unit increase in height (1 cm) will result in an increase of 0.05093 liters in the predicted fev value. The coefficient for genderM means that, given the height of an individual (or, for any height), the predicted value of fev for a male will be 0.10568 liters more than for a female. Model coefficients are sometimes refered to as the effect of the corresponding \\(X\\) variable on the predicted \\(Y\\) variable. So, 0.05093 is said to be the effect of height on the predicted fev, and 0.10568 is the effect of being male on the predicted fev. 1.2.1 Predicted values Using the equation we got in the previous section we can obtain predictions of the fev given height and genderM (coded as 0 for females, and 1 for males). Although we could use the equation to calculate ourselves the predictions of all the observations in the data frame, the predict() function (with the model object as argment) will do it for us. The result will be a vector with as many elements as rows in the dataframe used to fit the model (dnsin this case). Here we use head() to limit the output of the predictions to the first six cases in dns: predict(model1) %&gt;% head() 1 2 3 4 5 6 0.6287320 0.8833903 0.8833903 0.8833903 0.9852537 0.9852537 A graphical representation of the fitted model is shown in figure 1.2. Note the use of predict() in gf_line() to overlay predictions on the scatterplot of fev by height. As you see, the graphical appearance of model1 is that of two straight lines, one for males and another one for females, having identical slope but different intercept. library(ggformula) gf_point(fev ~ height, col = ~ gender, data =dns, alpha = 0.3) %&gt;% gf_line(predict(model1) ~ height, col = ~ gender, data = dns) Figure 1.2: Graphical representation of model 1 By default, function predict() computes predictions for the observations (rows) in the dataframe used to fit the model, but it can be used also to compute predictions for arbitrary values of the explanatory variables. To this end, we need to use the optional argument newdata, passing it a dataframe with the combinations of values of explanatory variables for which we want to compute the prediction. In this dataframe, the predictor variables should have the same name as in the dataset used to fit the model. Here we prepare a new dataframe with height values of 150, 160, 170, and 180 cm, for both boys and girls: # create dataframe with desired combinations of predictors (x) x &lt;- data.frame(height = rep(seq(150, 180, 10), each=2), gender = rep(c(&quot;M&quot;, &quot;F&quot;), each = 2)) x height gender 1 150 M 2 150 M 3 160 F 4 160 F 5 170 M 6 170 M 7 180 F 8 180 F Now we can use dataframe x to compute the predicted fev values for each of its combinations of height and sex values: predict(model1, newdata = x) 1 2 3 4 5 6 7 8 2.415154 2.415154 2.818794 2.818794 3.433787 3.433787 3.837427 3.837427 If desired, these predictions could be incorporated into dataframe x using predict() in a mutate() statement: x %&gt;% mutate(predicted_fev = predict(model1, newdata = x)) height gender predicted_fev 1 150 M 2.415154 2 150 M 2.415154 3 160 F 2.818794 4 160 F 2.818794 5 170 M 3.433787 6 170 M 3.433787 7 180 F 3.837427 8 180 F 3.837427 1.2.2 Residuals The residuals of a fitted model are estimates of the model error terms (\\(\\epsilon\\)) in the population equation. For any observation, the residual (\\(e\\)) is computed as the difference between the observed value (\\(Y\\)) and the value predicted by the model (\\(\\hat{Y}\\)): \\[e = Y - \\hat{Y}\\] Figure 1.3 shows the graphical representation of model1 (just as in figure 1.2), but now in two different panels for females (F) and males (M). The black lines are the same lines appearing in figure 1.2. For each observation, the vertical distance between the observed fev value (dots) and the corresponding predicted value (black lines) is displayed as a red line segment to represent its residual value. For all dots above the lines, the observed fev value is greater than the predicted value, and therefore the residual will be positive. Conversely, for all dots below the line, the observed value is lower than the predicted value, and therefore the residual will be negative. Any dot that lie exactly on the line have equal observed and predicted values, and therefore its residual value is zero. gf_point(fev ~ height | gender, data =dns, alpha = 0.2) %&gt;% gf_line(predict(model1) ~ height, data = dns) + geom_segment(aes(xend = height, yend = predict(model1), color = &quot;resid&quot;)) + scale_color_manual(values = c(resid = &quot;darkred&quot;), labels = c(resid = &quot;residuals&quot;)) Figure 1.3: Graphical representation of model 1 and residuals The residuals of the model can be obtained with function resid() applied to the model object model1, as done below (again, we use head() to limit the output to the residuals of the first six cases in dns): resid(model1) %&gt;% head() 1 2 3 4 5 6 0.44326801 -0.04439034 0.21860966 0.50560966 0.59174632 0.43274632 We can easily verify that residuals are nothing but the difference of observed and predicted values. Note that now we get exactly the same values listed above using resid(): obs_minus_pred &lt;- dns$fev - predict(model1) head(obs_minus_pred) 1 2 3 4 5 6 0.44326801 -0.04439034 0.21860966 0.50560966 0.59174632 0.43274632 1.3 Inference on the model coefficients It is important to understand that the equation obtained in the previous section 1.2 is an estimate of the population equation (that is, what we would get if we could fit the model with all individuals in the population). We may want to answer questions about the model coefficients in the population equation, such as: What are likely values of model coefficients in the population equation? What are likely values of the model predictions in the population equation? Could some model coefficients be equal to zero in the population equation? Could all model coeffcients be equal to zero in the population equation? The first two questions above are estimation questions, and the last two can be solved with appropriate significante tests. We address each of these questions in the following subsections. 1.3.1 Confidence intervals (CI) for the model coefficients Confidence intervals for the model coefficients can be easily obtained with function confint() applied to a model object: confint(model1) 2.5 % 97.5 % (Intercept) -5.69151579 -4.96903077 height 0.04856692 0.05329642 genderM 0.03760899 0.17374504 The output shows the lower (2.5%) and upper (99.5%) limits of the 95% CI for each coefficient in model1. The interpretation is as usual for a CI: we are 95% confident that (rounding to the fourth decimal): for any given gender, a unit increase in height (1 cm) will result in an increase of 0.0486 to 0.0533 liters in the predicted FEV. for any given height, the predicted value of FEV for a male will be between 0.0376 and 0.1737 liters more than for a female. CI’s can be computed for any desired confidence level using the argument level in confint(). For instance, here we compute the 99% CI for the model coefficients. Note that the confidence level has to be specified as a probability rather than as a percentage: confint(model1, level = 0.99) 0.5 % 99.5 % (Intercept) -5.80559378 -4.85495278 height 0.04782015 0.05404319 genderM 0.01611356 0.19524047 1.3.2 Confidence intervals (CI) for the model predictions A CI can also be obtained for each possible model prediction. Because a model prediction is interpreted as the expected or mean FEV for a given combination of height and gender, a CI for the prediction can be interpreted as a CI for the mean FEV, given the predictors. These can be computed using the optional argument interval in the predict() function, as done below (we use head() to limit the output to the first six cases in dns): predict(model1, interval = &quot;confidence&quot;) %&gt;% head() fit lwr upr 1 0.6287320 0.5340036 0.7234604 2 0.8833903 0.7985886 0.9681921 3 0.8833903 0.7985886 0.9681921 4 0.8833903 0.7985886 0.9681921 5 0.9852537 0.9042798 1.0662275 6 0.9852537 0.9042798 1.0662275 The result of the previous function call is a matrix, having as many rows as dns and three columns: fit (the predicted value), and lwrand upr corresponding to the lower and upper limits of the 95% CI for the prediction. These can be used to plot the confidence bands of the model equation, as done in figure 1.4. Note in the code below the sub-setting of the matrix produced by predict(), using [,1], [,2], and [,3] to get the first, second or third column of the matrix, respectively. gf_point(fev ~ height | gender, col = ~ gender, data =dns, alpha = 0.2) %&gt;% gf_line(predict(model1, interval = &quot;confidence&quot;)[,1] ~ height) %&gt;% gf_line(predict(model1, interval = &quot;confidence&quot;)[,2] ~ height, linetype = 2) %&gt;% gf_line(predict(model1, interval = &quot;confidence&quot;)[,3] ~ height, linetype = 2) Figure 1.4: Graphical representation of model 1 (solid lines) and 95% confidence bands (dashed lines) The confidence bands (dashed lines) in figure 1.4 mean that, in the population, the model could be any line we can draw within these limits. Note that the bands are not straight lines parallel to the estimated model (solid line), but are curved, so that the CI is wider for extremes values of height (such as 120 or 180 cm) than they are for values close to the mean of height (such as 150 cm). 1.3.3 Tests on the model coefficients Suppose we are doubtful about the relation between one of the explanatory variables (\\(X\\)) and the predicted variable \\(Y\\). For instance, we may wonder if gender is really related to fev. If gender was unrelated to the forced expiratory volume, then the slope of gender would be equal to zero in the population equation (meaning that gender has no effect on the FEV, or is unrelated to FEV). Therefore, we would like to conduct a significance test with the following hypotheses: \\[H_0: \\qquad \\beta_{genderM} = 0\\] \\[H_1: \\qquad \\beta_{genderM} \\ne 0\\] Such type of tests, sometimes called Wald tests, can be produced with function summary() applied to the model object model1: summary(model1) Call: lm(formula = fev ~ height + gender, data = dns) Residuals: Min 1Q Median 3Q Max -1.6550 -0.2498 -0.0003 0.2302 2.1046 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -5.330273 0.183930 -28.980 &lt;2e-16 *** height 0.050932 0.001204 42.301 &lt;2e-16 *** genderM 0.105677 0.034657 3.049 0.0024 ** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.4138 on 586 degrees of freedom Multiple R-squared: 0.764, Adjusted R-squared: 0.7632 F-statistic: 948.8 on 2 and 586 DF, p-value: &lt; 2.2e-16 The first part of the output (Residuals) shows summary statistics for the residuals of the model. The minimum residual is -1.655 liters, the maximum is 2.1046 liters, and the median is very close to zero. The mean of the residuals is not reported because it is always exactly zero (this is a property of GLS models). The Coefficients part of the output shows the estimates of the model coefficients (first column Estimate), their standard error (Std. Error), a t-statistic (t value) and the p value (Pr(&gt;|t|)). When the p value is too small, the null hypothesis stated above is rejected. In this case, all coefficients show small p values and therefore we can conclude that they are all different from zero in the population equation. In other words, for coefficients other than the intercept, a significant result in this test is interpreted as evidence of a linear relation of FEV with the corresponding explanatory variable. In this case, the Wald tests provide evidence that both height and gender are related to FEV. Last, the output provides an estimate for the standard deviation of the residuals \\(\\sigma\\) (Residual standard error), some measures of how well the model fits the data (Multiple R-squared and Adjusted R-squared), and a test (F-statistic and p-value) of the null hypothesis that all \\(\\beta\\) coefficients are zero in the population regression equation, i.e., that all explanatory variables are linearly independent of the \\(y\\) variable. 1.4 Goodness-of-fit In our sample data, the FEV values (\\(Y\\)) range from 0.791 to 5.793 liters, with mean 2.566 liters. If we want to predict the FEV value for a particular individual and we have no information on his or her height and gender, our best guess would be the mean value of FEV in the whole sample, 2.566 liters. The error of such a prediction would be the difference between the observed FEV for this individual (\\(Y\\)) an the mean of FEV (\\(\\bar{Y}\\)), that is, \\(Y - \\bar{Y}\\). However, if we know the height and the gender of this individual, we can get a better prediction using model1 (\\(\\hat{Y}\\) = -5.33027 + 0.05093 height + 0.10568 genderM), reducing the prediction error to \\(Y - \\hat{Y}\\). In fact, the error of the initial prediction based on the sample mean (\\(Y - \\bar{Y}\\)) can be decomposed in two parts, as follows: \\[\\begin{equation} Y - \\bar{Y} = (Y - \\hat{Y}) + (\\hat{Y} - \\bar{Y}) \\tag{1.2} \\end{equation}\\] \\((Y - \\hat{Y})\\) is the residual of model1, that is, the prediction error when using model1 to get a better prediction \\(\\hat{Y}\\). \\((\\hat{Y} - \\bar{Y})\\) is the part of the error in our initial prediction that can be explained by model1 (i.e., by the height and the gender of the individual). This decomposition has been represented in figure 1.5 for a couple of cases. The black solid lines are predictions made by model1. The black dashed lines indicate the overall mean of FEV in the sample. Blue segments indicate the deviation from the overall mean that are explained by the model, and red segments indicate the residuals, that is, the part of deviation from the overall mean that are not explained by the model. Figure 1.5: Graphical representation of the decomposition of deviations from overall mean of FEV according to model 1 It can be shown that the equality of equation (1.2) holds after squaring the three differences and summing over all the (\\(n\\)) individuals, that is \\[\\begin{equation} \\sum_{i=1}^{n} (Y - \\bar{Y})^2 = \\sum_{i=1}^{n}(Y - \\hat{Y})^2 + \\sum_{i=1}^{n}(\\hat{Y} - \\bar{Y})^2 \\tag{1.3} \\end{equation}\\] The three terms in equation (1.3) are called sums of squares (or sometimes, variation), and measure (from left to right): the total variation of \\(Y\\) around its mean \\(\\bar{Y}\\), the variation of the residuals, and the variation of the predictions \\(\\hat{Y}\\) around the mean \\(\\bar{Y}\\). Equation (1.3) shows that the total sum of squares can be decomposed in two parts: the residual sum of squares, and the variation of the model predictions. Let’s verify this for model1: ## sum of squares of ss_total &lt;- sum((dns$fev - mean(dns$fev))^2) # variation of FEV around its mean ss_resid &lt;- sum(resid(model1)^2) # variation of residuals ss_model &lt;- sum( (predict(model1) - mean(dns$fev))^2 ) # variation of predictions ss_total [1] 425.3515 ss_resid [1] 100.3642 ss_model [1] 324.9872 ss_resid + ss_model == ss_total [1] TRUE Clearly, the greater the variation of the predictions (blue segments in figure 1.5) and the smaller the variation of the residuals (red segments), the better the model will be. Then, a natural measure of how well a model fits the data will be given by the proportion of the total variation that is explained by the model, that is \\[R^2 = \\frac{\\sum_{i=1}^{n}(\\hat{Y} - \\bar{Y})^2}{\\sum_{i=1}^{n} (Y - \\bar{Y})^2}\\] This quantity, called coefficient of determination R-squared, measures how good a model is: the higher the \\(R^2\\) value, the better a model fits the data (or predicts \\(Y\\)). Since all terms in equation (1.3) are necessarily non-negative (because of the squares), and the top of the previous formula is part of the bottom, the theoretical range of \\(R^2\\) is 0 to 1. A value of 0 would imply that the model explains no variation at all. A value of 1 would imply that the model explains all the variation (and there is no residual variation), in which case the model would fit the data perfectly. It is easy to verify that the previous formula for \\(R^2\\) gives the same result given by summary(model1): ss_model / ss_total # R-squared computed from sums of squares [1] 0.764044 summary(model1)$r.squared # R-squared as reported by summary(model1) [1] 0.764044 Thus, model1 explains 76.4 % of the total variation of FEV values. Consequently, the residual variation is 23.6 % of the total variation of FEV. Figure 1.6 shows the centered distributions1 of observed and predicted FEV values, and of the residuals. Figure 1.6: centered distribution of observed and predicted FEV, and residuals Although \\(R^2\\) is a goodness-of-fit measure with an interesting interpreteation (i.e., the proportion of variance of the outcome variable that is explained by the model), it has a limitation when used to compare different models. This limitation is due to the fact that \\(R^2\\) will increase as we add explanatory variables to a model, even if some of these variables do not improve much its predictive capacity. Therefore, when comparing two models with a different number of predictors, the more complex model will always have a higher \\(R^2\\) value, and might be judged to be better than simpler models. However, simple models (or parsimonious models, as they are sometimes called) are in general preferable to complex models, particularly when the later includes some predictors with no predictive capacity. To avoid this problem, a version of \\(R^2\\) that penalizes model complexity is given by a measure called adjusted \\(R^2\\), which is computed as: \\[R^2_{adj} = 1-\\frac{(1-R^2)(n-1)}{n-k-1}\\] where \\(n\\) is the number of observations and \\(k\\) is the number of coefficients in the model, excluding the intercept. As noted in the previous section, the value of both \\(R^2\\) and \\(R^2_{adj}\\) are shown at the end of the output provided by summary(). \\(R^2_{adj}\\) provides a measure better than \\(R^2\\) to compare models with a different number of predictors. \\(R^2_{adj}\\) may be actualy lower if the additional predictors of the more complex model have little capacity of improving predictions. 1.5 Assessing assumptions The validity of the inferential analyses discussed in section 1.3 depend on the following assumptions: The relation of \\(Y\\) and \\(X\\) variables is as specified by the model, i.e., the model is appropriate. Observations are independent from each other. The prediction errors follow a normal distribution with constant variance. The last two assumptions are sometimes expressed by saying that errors are independent and identically distributed (iid) random variables following a normal distribution with constant variance, which is concisely expressed as \\(\\quad \\epsilon \\sim N(0, \\sigma)\\). The independence of observations is generally met when each observation corresponds to a different individual, which is the case in the dns dataset. However, this assumption would be violated if some individual(s) contributed with more than one observation (i.e, when two or more points in a scatterplot correspond to the same individual). The condition of independent observations is sometimes expressed by saying that the error terms of different observations are uncorrelated, i.e., \\(Cov(\\epsilon_i, \\epsilon_j) = 0\\) for all \\(i \\ne j\\). The remaining assumptions can be verified by inspecting some plots of the residuals that can be easily produced with the base R function plot() applied to the model object, and a further argument to indicate the type of plot we want (out of six plot types; see ?plot() for more details). A couple of such plots are produced below for model1: a residual vs predicted (or fitted) values, and a normal quantile-quantile plot of the residuals. Because these plots are created with the base R plotting system, the par() function is used previously to set two graphic slots and the plot margins. # to set two slots (side by side) in the graphic output, and the plot margins par(mfrow = c(1,2), mar = c(4,7,2,2)) plot(model1, 1) # plot 1: residuals vs predicted values plot(model1, 2) # plot 2: Normal QQ plot of residuals Figure 1.7: Diagnostic plots for model1 When the model assumptions hold, a zero-centered horizontal band (or ellipse) should be seen in the Residual vs Fitted plot of figure 1.7 (left), with about the same vertical variability for all fitted values. In this case we see a slightly U-shaped pattern (highlighted by the red line) instead of a horizontal band, which means that the relation of fev and height is not linear. In addition, the vertical variability of the residuals increases from left to right, suggesting that their variance is not constant, a phenomenon called heteroscedasticity. In fact, both the non-linearity and the heteroscedasticity are already visible in the scatterplot of figure 1.1 (center), but the residual vs fitted plot makes them even more evident. The Normal QQ plot 1.7 (right) shows a pretty good approximation to the normal distribution (most points are over the dashed line representing a perfect normal distribution), though there is a slight deviation in the tails of the distribution, where some outliers are identified by the row number in dataframe dns. Non-linearities and heteroscedasticity are common phenomena when analyzing real data. In the next sections we see how to cope with them and fit better models. 1.5.1 Fixing non-linearity: polynomials There are several ways we could try to address a non-linear relationship of \\(Y\\) with an explanatory variable \\(X\\) . One such way is to use polynomials to accommodate the non-linearity. Let’s try to fit a model with a linear and a quadratic term for height, by using function poly(). This function builds a polynomial for the variable indicated as first argument, and the degree of the polynomial as second argument. Here, we fit a quadratic function (a second degree polynomial) for height and then show its graphical representation by plotting predicted values over the scatterplot: model2 &lt;- lm(fev ~ poly(height,2) + gender, data = dns) gf_point(fev ~ height, col = ~ gender, data =dns, alpha = 0.3) %&gt;% gf_line(predict(model2) ~ height, col = ~ gender, data = dns) Figure 1.8: Graphical representation of model2 Figure 1.8 shows that a second degree polynomial on height follows more closely the change of FEV as height increases. The estimates of the model coefficients are now: model2 Call: lm(formula = fev ~ poly(height, 2) + gender, data = dns) Coefficients: (Intercept) poly(height, 2)1 poly(height, 2)2 genderM 2.53928 17.87793 2.87018 0.05104 Therefore, the estimated equation is predicted fev = 2.53928 + 17.87793 height + 2.87018 height^2 + 0.05104 genderM By inspecting the diagnostic plots for model2 we see we have fixed the non-linearity problem, but heteroscedasticity persists: par(mfrow = c(1,2), mar = c(4,7,2,2)) plot(model2, 1) plot(model2, 2) Figure 1.9: Diagnostic plots for model 2 1.5.2 Fixing heteroscedasticity Heteroscedasticity is sometimes fixed using a logarithmic transformation of the \\(Y\\) variable. Let’s see what happens if we plot the logarithm of FEV against height: gf_point(log(fev) ~ height, col = ~ gender, data =dns, alpha = 0.3) Figure 1.10: Scatterplot of log(FEV) by height and gender In figure 1.10, the relation of log(fev)and height looks linear, with pretty constant variability of log(fev) values across height values. So, by taking logs of FEV, we got rid of both the non-linearity and heteroscedasticity. Let’s then fit a third model using log(fev) instead of fev as dependent variable, and see what are the estimates of the model coefficients: model3 &lt;- lm(log(fev) ~ height + gender, data = dns) model3 Call: lm(formula = log(fev) ~ height + gender, data = dns) Coefficients: (Intercept) height genderM -2.2771 0.0205 0.0168 The estimated equation is now: predicted log(fev) = -2.2771 + 0.0205 height + 0.0168 genderM Diagnostic plots (figure 1.11 look better now: the residual vs fitted values shows a horizontal ellipse with no hint of heteroscedasticity, but the normal QQ plot shows just a couple of outliers in the lower tail. par(mfrow = c(1,2), mar = c(4,7,2,2)) plot(model3, 1) plot(model3, 2) Figure 1.11: Diagnostic plots of model 3 1.5.3 Influence measures Outliers may influence the estimates of the model parameters (i.e., distort these estimates). Weather or not they do it in a non-negligible way depends on how many they are, how far they are, and the sample size; but it may be difficult to judge their influence from the graphics of figure 1.11. To judge this, a measure of influence called Cook’s distance is useful. The Cook’s distance \\(D_i\\), which is computed for each observation (\\(i = 1, 2, ..n\\)), is a (normalized) measure of the overall change in the predicted values that would result from removing this observation. \\[D_i = \\frac{\\sum_{j=1}^{n} (\\hat{Y}_j - \\hat{Y}_{j(i)})^2}{k \\; MSE}\\] where, \\(\\hat{Y}_j\\) is the predicted value for observation \\(j\\) \\(\\hat{Y}_{j(i)}\\) is the predicted value for observation \\(j\\) after removing observation \\(i\\) \\(k\\) is the number of coefficients in the model, and \\(MSE\\) is the mean squared error (the mean of the squared residuals). Cook’s distances can be inspected in a further diagnostic plot we can get with plot(), passing 4 as second argument: plot(model3, 4) Figure 1.12: Cook’s distances by observation index As a rule of thumb, outlying observations will have little influence if the Cook’s distance is below 0.5 (or even below 1). Therefore, no worries in this case, since the highest value is about 0.05 (for observation 111). 1.6 Probability intervals (or prediction intervals) In section 1.3.2 we saw how to get CIs for the model predictions, which refer to the expected or mean values \\(\\hat{Y}\\) for cases with particular values of the explanatory variables. But suppose we now want to compute a probability interval, an interval that will include individual \\(Y\\) values with a given probability. This type of interval can be also computed with function predict(), but specifying the argument interval = \"prediction\" as done below (we use head() to limit the output to the first six cases in dns): predict(model3, interval = &quot;prediction&quot;) %&gt;% head() fit lwr upr 1 0.1219614 -0.17245402 0.4163768 2 0.2244847 -0.06953864 0.5185081 3 0.2244847 -0.06953864 0.5185081 4 0.2244847 -0.06953864 0.5185081 5 0.2654941 -0.02838958 0.5593777 6 0.2654941 -0.02838958 0.5593777 The output shows the predictions (fit) and lower (lwr) and upper (upr) limits of 95% probability intervals. By default, predict()computes 95% probability or confidence intervals, but this can be changed with its optional argument level. Below we plot the 95% probability bands and also the 95% CI bands for model3 to see how different they are: probability bands are much wider than confidence bands. gf_point(log(fev) ~ height | gender, col = ~ gender, data =dns, alpha = 0.2) %&gt;% gf_line(predict(model3, interval = &quot;prediction&quot;)[,1] ~ height) %&gt;% gf_line(predict(model3, interval = &quot;prediction&quot;)[,2] ~ height, linetype = 2) %&gt;% gf_line(predict(model3, interval = &quot;prediction&quot;)[,3] ~ height, linetype = 2) %&gt;% gf_line(predict(model3, interval = &quot;confidence&quot;)[,2] ~ height, linetype = 2, color = &quot;black&quot;) %&gt;% gf_line(predict(model3, interval = &quot;confidence&quot;)[,3] ~ height, linetype = 2, color = &quot;black&quot;) Figure 1.13: Graphical representation of model 3 (solid lines), 95% CI bands (black dashed lines) and 95% probability bands (colored dashed lines) Because model3 was fitted on the log of FEV, predictions, confidence intervals and probability intervals computed from this model are log-transformed values of FEV (see the vertical axis of figure 1.13). However, the log transformation can be reverted by exponentiating the results (i.e., the predictions, their CI’s and predictiony intervals): gf_point(fev ~ height | gender, col = ~ gender, data =dns, alpha = 0.2) %&gt;% gf_line(exp(predict(model3, interval = &quot;prediction&quot;)[,1]) ~ height) %&gt;% gf_line(exp(predict(model3, interval = &quot;prediction&quot;)[,2]) ~ height, linetype = 2) %&gt;% gf_line(exp(predict(model3, interval = &quot;prediction&quot;)[,3]) ~ height, linetype = 2) %&gt;% gf_line(exp(predict(model3, interval = &quot;confidence&quot;)[,2]) ~ height, linetype = 2, color = &quot;black&quot;) %&gt;% gf_line(exp(predict(model3, interval = &quot;confidence&quot;)[,3]) ~ height, linetype = 2, color = &quot;black&quot;) Figure 1.14: Transforming back the results to the original FEV scale The 95% probability intervals for individual FEV values shown in figure 1.14 could be used as reference normal ranges for FEV, given the height and gender of an individual (as we supposed was the goal of the analysis in section 1.1). 1.7 Models with two continuous predictors The models fitted in previous sections were all based on height and gender, which are quantitative and categorical variables, respectively. That is why the graphical representation of these models was a couple of lines, one for each gender, describing the relation of FEV and height. Let’s now fit a model with two quantitative predictors, such as height and age: model4 &lt;- lm(log(fev) ~ height + age, data = dns) model4 Call: lm(formula = log(fev) ~ height + age, data = dns) Coefficients: (Intercept) height age -1.93120 0.01677 0.02493 Therefore, the estimated equation is: predicted fev = -1.93120 + 0.01677 height + 0.02493 age When a model is based on two quantitative explanatory variables, the equation corresponds to a plane. Figure 1.15 shows the regression plane of model4. This plane cuts the \\(Y\\) axis at the intercept (-1.93120), and is tilted according to the slopes of height and age (in the direction of their respective axes). Figure 1.15: Graphical representation of model 4: regression plane If a model has more than two quantitative predictors, the plane generalizes to a hyperplane that cannot be represented graphically in a 2D screen. But let’s see what happens if we addition a categorical variable like gender to model4: model5 &lt;- lm(log(fev) ~ height + age + gender, data = dns) model5 Call: lm(formula = log(fev) ~ height + age + gender, data = dns) Coefficients: (Intercept) height age genderM -1.90378 0.01642 0.02609 0.02840 Therefore, the estimated equation is: predicted fev = -1.90378 + 0.01642 height+ 0.02609 age + 0.02840 genderM. Now model5 are two parallel regression planes, as shown in figure 1.16. While the two planes may look the same, they are actually different. The intercept for the plane of females is -1.90378, while that of males is -1.875385. Guess why? Figure 1.16: Graphical representation of model 5: parallel regression planes for males an females 1.8 Comparing models In previous sections we fitted three different models for log(fev): model3, model4 and model5. A first step in comparing these three models is to look at their \\(R^2_{adj}\\) values. We can easily pick the value of \\(R^2_{adj}\\) from the summary of a model, since summary() produces a list and one of its elements is precisely the \\(R^2_{adj}\\) statistic, identified as adj.r.squared: summary(model3) %&gt;% names() # elements of summary() result? [1] &quot;call&quot; &quot;terms&quot; &quot;residuals&quot; &quot;coefficients&quot; [5] &quot;aliased&quot; &quot;sigma&quot; &quot;df&quot; &quot;r.squared&quot; [9] &quot;adj.r.squared&quot; &quot;fstatistic&quot; &quot;cov.unscaled&quot; Here we pick the adj.r.squared element of each model summary: summary(model3)$adj.r.squared [1] 0.7984583 summary(model4)$adj.r.squared [1] 0.81268 summary(model5)$adj.r.squared [1] 0.8141139 According to \\(R^2_{adj}\\), model5 is better. However, particularly when the improvement in \\(R^2_{adj}\\) is small, we may wonder if it is significantly better. This can be investigated with function anova(), by passing two (or more) models as arguments, provided they are nested models. Two models are nested if one of them is contained in the other. Here we compare model4 and model5: anova(model4, model5) Analysis of Variance Table Model 1: log(fev) ~ height + age Model 2: log(fev) ~ height + age + gender Res.Df RSS Df Sum of Sq F Pr(&gt;F) 1 586 12.075 2 585 11.962 1 0.11288 5.5202 0.01913 * --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The result shows the residual sum of squares (RSS) and the residual degrees of freedom (Res.Df) for each model (the residual degrees of freedom of a model is the sample size minus the number of parameters in the model). Then, for the second model in the output, the change in both degrees of freedom (Df) and RSS (Sum of Sq) with respect to the first model are shown, as well as the result of an F-test (F) assessing the significance (Pr(&gt;F)) in the reduction of the RSS. In this case, the low p value indicates a significant reduction in the RSS of the second model (model 5) as compared to the first model (model4). Therefore, we can conclude that model5fits the data significantly better than model4. 1.9 Confounding Suppose we now want to investigate the effect of smoking on the FEV. Figure 1.17 (left) shows the boxplot of fev by smoke in dataframe d. Surprisingly enough, smokers tend to show higher FEV values than non-smokers, which is counter intuitive. plot1 &lt;- gf_boxplot(fev ~ smoke, data = d) + coord_flip() plot2 &lt;- gf_boxplot(age ~ smoke, data = d) + coord_flip() plot1 + plot2 Figure 1.17: FEV and age in smokers and non-smokers However, it is the case that smokers are older than non-smokers as shown in figure 1.17 (right), and this might be the reason why, when comparing the fev values in smokers and non-smokers without taking age into account, we see higher FEV values in non-smokers. This can be verified by fitting a couple of models for fev, one of them including only smoke as predictor, and the other including both smoke and age: lm(fev ~ smoke, data =d) Call: lm(formula = fev ~ smoke, data = d) Coefficients: (Intercept) smokeyes 2.5661 0.7107 lm(fev ~ smoke + age, data =d) Call: lm(formula = fev ~ smoke + age, data = d) Coefficients: (Intercept) smokeyes age 0.3674 -0.2090 0.2306 In the first model, the estimated effect of smoking is 0.7107, suggesting that the expected FEV is 0.7107 liters higher in smokers than in non-smokers. However, in the second model the expected FEV in smokers given the age, is 0.2090 liters lower in non-smokers than in smokers. The change in the estimated effect of smoking from the first to the second model reflects the fact that, in the former, the effect of age is not taken into account, and because age is related to smoking, its effect is confounded with that of smoking. In the second model, the effect of gender is adjusted by age, or estimated after partialling out the effect of age. As seen in this example, when we want to assess the effect of a variable \\(X_1\\) on an outcome variable \\(Y\\), modeling provides a way to address potential confounding by including the potential confounder(s) as additional explanatory variable(s) in the model. If the effect of \\(X_1\\) does not changes appreciably, there is no confounding. However, an appreciable change in the effect of \\(X_1\\) is an indication of confounding. 1.10 Interaction (effect modification) The National Health and Nutrition Examination Survey (NHANES) is a program of studies designed to assess the health and nutritional status of adults and children in the United States. The survey examines a nationally representative sample of about 5,000 persons each year. The dataset calcium.RData is an extract of the 2009-2010 NHANES survey containing complete data of calcium concentration in blood (mg/dl), gender, age, and vitamin D (nmol/l) for adults older than 25 years. Download calcium.RData Let’s get the data file with load(), and explore how calcium concentration values relate to age and gender. load(&quot;data/calcium.RData&quot;) gf_point(calcium ~ age | gender, data = calcium, color = ~gender, alpha = 0.2) Figure 1.18: Calcium concentration (mg/dl) by age and gender Figure 1.18 shows a very slight decrease of calcium concentration with age in males, but a slight increase in females. Fitting a linear model with age and gender as explanatory variables implies to fit two lines, one for each gender, having different intercepts but the same slope, so that it cannot accommodate what we observe in this plot. To allow for different slopes, we need to add an interaction term to the model. An interaction term is a product of two explanatory variables, such as age and gender in this case. In general, a model with a term of interaction between two explanatory variables \\(X_1\\) and \\(X_2\\) will be of the form: \\[Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_{12} X_1 X_2 + \\epsilon\\] Suppose \\(X_1\\) is a continuous variable (like age), and \\(X_2\\) is an indicator for a dichotomous variable (like gederfemale, coded as 0 for males and 1 for females). Then, the model simplifies to the following equations for males and females: For males: \\[Y \\quad = \\quad \\beta_0 + \\beta_1 X_1 + \\beta_2 \\times 0 + \\beta_{12} X_1 \\times 0 + \\epsilon \\quad = \\quad\\beta_0 + \\beta_1 X_1 + \\epsilon\\] For females: \\[Y \\quad = \\quad \\beta_0 + \\beta_1 X_1 + \\beta_2 \\times 1 + \\beta_{12} X_1 \\times 1 + \\epsilon \\quad = \\quad (\\beta_0 + \\beta_2) + (\\beta_1 + \\beta_{12}) X_1 + \\epsilon \\] Both equations above are straight lines, but they have a different intercept and a different slope. Interaction terms can be specified in function lm() in two different ways. One of them is to specify the two explanatory variables as usual (X1 + X2), and then add an additional term of the form X1:X2. The following fits a model with an interaction between age and gender using this syntax (note the colon between ageand gender in the last term of the formula): m1 &lt;- lm(calcium ~ age + gender + age:gender, data = calcium) Alternatively, we can specify just the two explanatory variables, but using an asterisk instead of a plus sign between them (X1*X2), as in the following code (note the asterisk between age and gender): m1 &lt;- lm(calcium ~ age * gender, data = calcium) In both cases we are fitting the same model, with main effects for age and gender, plus a term of interaction between them. Here we summarize the fit of this model: summary(m1) Call: lm(formula = calcium ~ age * gender, data = calcium) Residuals: Min 1Q Median 3Q Max -1.9930 -0.2389 -0.0116 0.2116 2.6834 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 9.5726854 0.0253009 378.354 &lt; 2e-16 *** age -0.0024783 0.0004569 -5.424 6.1e-08 *** genderfemale -0.4282551 0.0348558 -12.286 &lt; 2e-16 *** age:genderfemale 0.0076006 0.0006338 11.993 &lt; 2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.363 on 5091 degrees of freedom Multiple R-squared: 0.03305, Adjusted R-squared: 0.03248 F-statistic: 58 on 3 and 5091 DF, p-value: &lt; 2.2e-16 The Wald test for the interaction term age:genderfemale produces a very small p value, providing evidence of an interaction between age and gender. The estimated equation is: predicted Ca = 9.5726854 -0.0024783 age -0.4282551 genderfemale + 0.0076006 age x genderfemale which for males (genderfemale = 0) it simplifies to: predicted Ca = 9.5726854 -0.0024783 age and for females (genderfemale = 1), it simplifies to: predicted Y = (9.5726854 -0.4282551) + (0.0076006 - 0.0024784) age = 9.14443 + 0.0051222 age Note that the slope is negative for males, but positive for females. A graphical representation of this model is shown in figure 1.19. gf_point(calcium ~ age | gender, data = calcium, color = ~gender, alpha = 0.1) %&gt;% gf_line(predict(m1) ~ age | gender, color = &quot;black&quot;) Figure 1.19: Graphical representation of model m1 In the example we have considered, there is an interaction of a continuous and a categorical variable (age and gender, respectively). However, interactions may arise between any type of variables, like two continuous variables, or two categorical variables. In any case, the way to fit and test for interaction terms is exactly the same. Sometimes, when there is an interaction of two explanatory variables, the term effect modifier is applied to one of these variables. For instance, we could say that gender is a modifier of the effect of age on the calcium concentration, since the coefficient of age (that is, the effect of age) is different in males and females. Resources Elements of statistical learning, by T. Hastie, R. Tibshirani and J. Friedman, is a very good book on both classical and modern modeling methods, such as random forest, support vector machines or neural networks. Regression modeling strategies, by F.Harrell, is another very good book on modeling (with R). Be aware of common problems you may find when fitting a GLM (or othe types of multivariate model): Be aware of heteroscedasticity and how to deal with it. Be aware of what is colinearity, how to detect it, and how to deal with it. Be aware of autocorrelated errors and how to deal with them. Be aware of how to detect and deal with outliers in regression. Be aware of other pitfalls such as overfitting and extrapolation. For influence measures other than the Cook’s distance, see this article, and see also the help of influence.measures() in R (?influence.measures). When there are many potential predictor variables, the number of possible models can be very large, and it is not feasible to fit and assess them one at a time. Read this introduction to automated model selection methods. A post on different types of regression analysis you may want to know about. Exercises Figure 1.2 shows the graphical representation of model 1, in which fev is expressed as a linear function of height and gender: What is the model equation? Why do you think the graphical representation of this model are two parallel straight lines? Hint: simplify the model equation for a male (genderM = 1) and for a female (genderM = 0). Use the simplified equations to predict the FEV expected for a boy with body height = 170 cm, and for a girl having the same height. What is the difference between both predictions? Compare your predictions with those obtained with predict(). Plot the 95% probability intervals for model1 on fev and for model3 on log(fev). In which of the two models the probability intervals seem to work better (i.e., include about 95% of the cases, for any given combination of height and gender)? Regarding model 5, in which log(fev) is expressed as a linear function of height, age and gender, why do you think the graphical representation of this model in figure 1.15 are two parallel planes? Hint: simplify the model equation for a male (genderM = 1) and for a female (genderM = 0). Produce diagnostic plots for modle5. Do you think that the model assumptions are reasonable? Using dataframe d (including smokers), fit a model for log(fev) with height, age, gender and smoke as predictors, and answer the following questions: According to Wald tests, are all predictor variables necessary? What is the proportion of variation of log(fev) explained by this model? Compare this model to model5 fitted on all cases in d. Which one best fits the data? Is there a significant improvement in the model fit by adding smoke to model5? Dataframe Births78 in the mosaic package contains the number of births registered in the USA in 1978. Inspect a scatterplot of births by date. Taking into account that each dot is a day, what do you think is the reason for the two-wave pattern? (Hint: are the two waves equally crowded?) Compute a new variable day taking values “Work”, “Sat” or “Sun” (it should be a factor, with levels defined in this order), and repeat the previous plot using color for day. Is it now clear what is the reason for the two waves? There are a few “Work” days behaving much like weekends. Can you anticipate an explanation for this? Fit a first model (m1) for births using day and a polynomial of degree 5 for month. Produce a graphical representation of this model. Why do you think it takes the form of a step function? Fit a second model (m2) similar to the previous one, but using day_of_year instead of month; produce its graphical representation and compare it to the previous one. Which one best fits the data? Run the following code to create a new variable identifying working days that were bank holidays in 1978, fit a third model (m3) adding this variable to m2, and compare them. Is there a significant improvement in fit after adding variable holiday to the predictors? h &lt;- as.Date(c(&quot;1978-01-02&quot;, &quot;1978-02-20&quot;, &quot;1978-05-29&quot;, &quot;1978-07-04&quot;, &quot;1978-09-04&quot;, &quot;1978-11-11&quot;, &quot;1978-11-23&quot;, &quot;1978-12-25&quot;)) d &lt;- d %&gt;% mutate(holiday = ifelse(date %in% h &amp; day == &quot;Work&quot;, &quot;Yes&quot;, &quot;No&quot;)) What is the proportion of the number of births variance explained by model m3? Assess the assumptions of model m3 by producing diagnostic plots. Do you think the assumptions are reasonable? Do you think the assumption of independence of observations is reasonable? A centered distribution is what results from subtracting the mean from all its values. For example, \\(Y-\\bar{Y}\\) is the centered distribution of \\(Y\\).↩︎ "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
